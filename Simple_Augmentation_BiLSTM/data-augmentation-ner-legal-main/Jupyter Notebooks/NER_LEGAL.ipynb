{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jW41BmVOIBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b552a20-6f88-4c6d-e408-fb60a7ce0103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/685-NER/data-augmentation-ner-legal-main/\")"
      ],
      "metadata": {
        "id": "UMLVrR1kJv1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to BIO Form\n",
        "!python ner_scripts/preprocess_wikiann.py --in_file ./data/raw_test/NER_DEV_JUDGEMENT.json --out_file ./data/bio_format/NER_DEV_JUDGEMENT.txt\n",
        "!python ner_scripts/preprocess_wikiann.py --in_file ./data/raw_test/NER_DEV_PREAMBLE.json --out_file ./data/bio_format/NER_DEV_PREAMBLE.txt\n",
        "!python ner_scripts/preprocess_wikiann.py --in_file ./data/raw_train/NER_TRAIN_JUDGEMENT.json --out_file ./data/bio_format/NER_TRAIN_JUDGEMENT.txt\n",
        "!python ner_scripts/preprocess_wikiann.py --in_file ./data/raw_train/NER_TRAIN_PREAMBLE.json --out_file ./data/bio_format/NER_TRAIN_PREAMBLE.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB9ogc_KKGfI",
        "outputId": "b593267f-714b-4664-a9c7-8672fff70372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save to ./data/bio_format/NER_DEV_JUDGEMENT.txt\n",
            "Save to ./data/bio_format/NER_DEV_PREAMBLE.txt\n",
            "Save to ./data/bio_format/NER_TRAIN_JUDGEMENT.txt\n",
            "Save to ./data/bio_format/NER_TRAIN_PREAMBLE.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge Files to create Train and Test\n",
        "with open('./data/bio_format/NER_DEV_JUDGEMENT.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    NER_DEV_JUDGEMENT = f.readlines()\n",
        "    f.close()\n",
        "with open('./data/bio_format/NER_DEV_PREAMBLE.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    NER_DEV_PREAMBLE = f.readlines()\n",
        "    f.close()\n",
        "with open('./data/bio_format/NER_TRAIN_JUDGEMENT.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    NER_TRAIN_JUDGEMENT = f.readlines()\n",
        "    f.close()\n",
        "with open('./data/bio_format/NER_TRAIN_PREAMBLE.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    NER_TRAIN_PREAMBLE = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "test = NER_DEV_JUDGEMENT + NER_DEV_PREAMBLE\n",
        "train = NER_TRAIN_JUDGEMENT + NER_TRAIN_PREAMBLE\n",
        "split_index = int(len(train) * 0.30)\n",
        "dev = train[:split_index]\n",
        "train = train[split_index:]\n",
        "\n",
        "print('Length of Train, Test and Dev - ', len(train), len(test), len(dev))\n",
        "\n",
        "import os\n",
        "#os.mkdir('./src/datasets/___1.1/')\n",
        "\n",
        "with open('./src/datasets/___1.1/test.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    for line in test:\n",
        "        f.write(line)\n",
        "    f.close()\n",
        "with open('./src/datasets/___1.1/train.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    for line in train:\n",
        "        f.write(line)\n",
        "    f.close()\n",
        "with open('./src/datasets/___1.1/dev.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    for line in dev:\n",
        "        f.write(line)\n",
        "    f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSo-n0ZoKJUC",
        "outputId": "5a8e5d58-c278-49c3-9d3c-b8c6c9a37525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Train, Test and Dev -  332332 49334 142428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ewvc7QYsKLti",
        "outputId": "9577499b-1474-4449-d438-c46374488dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair==0.10 (from -r requirements.txt (line 1))\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.7/322.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting BackTranslation==0.3.1 (from -r requirements.txt (line 2))\n",
            "  Downloading BackTranslation-0.3.1-py3-none-any.whl (8.9 kB)\n",
            "Collecting beautifulsoup4==4.11.1 (from -r requirements.txt (line 3))\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepl==1.6.0 (from -r requirements.txt (line 4))\n",
            "  Downloading deepl-1.6.0-py3-none-any.whl (31 kB)\n",
            "Collecting fasttext==0.9.2 (from -r requirements.txt (line 5))\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim==4.2.0 (from -r requirements.txt (line 6))\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mysql-connector-python==8.0.29 (from -r requirements.txt (line 7))\n",
            "  Downloading mysql_connector_python-8.0.29-cp310-cp310-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.0.53 (from -r requirements.txt (line 8))\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting SoMaJo==2.2.1 (from -r requirements.txt (line 9))\n",
            "  Downloading SoMaJo-2.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from -r requirements.txt (line 10))\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.19.1 (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-translate==2.0.1 (from -r requirements.txt (line 12))\n",
            "  Downloading google_cloud_translate-2.0.1-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Collecting segtok>=1.5.7 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (3.7.1)\n",
            "Collecting mpld3==0.3 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (1.2.2)\n",
            "Collecting sqlitedict>=1.6.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting bpemb>=0.3.2 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (0.8.10)\n",
            "Collecting langdetect (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (4.9.2)\n",
            "Collecting ftfy (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.95 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading sentencepiece-0.1.95.tar.gz (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konoha<5.0.0,>=4.0.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting janome (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gdown==3.12.2 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting more-itertools~=8.8.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Collecting googletrans==4.0.0rc1 (from BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from BackTranslation==0.3.1->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.11.1->-r requirements.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl==1.6.0->-r requirements.txt (line 4)) (2.27.1)\n",
            "Collecting pybind11>=2.2 (from fasttext==0.9.2->-r requirements.txt (line 5))\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->-r requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->-r requirements.txt (line 6)) (6.3.0)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mysql-connector-python==8.0.29->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.1->-r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.15.0 (from google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair==0.10->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.48.2)\n",
            "Collecting google-auth<3.0dev,>=1.25.0 (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->flair==0.10->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->flair==0.10->-r requirements.txt (line 1)) (4.5.0)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0 (from konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0 (from konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair==0.10->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (16.0.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair==0.10->-r requirements.txt (line 1)) (0.2.6)\n",
            "Collecting cachetools<5.0,>=2.0.0 (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1)) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (0.5.0)\n",
            "Building wheels for collected packages: fasttext, sacremoses, gdown, googletrans, mpld3, sentencepiece, sqlitedict, langdetect, overrides\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393242 sha256=519a63a3b1eb6cf06a0e316da1d202d98037317edd59efb03b214a683966ab41\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=958a415b5bab89c35e8810171ce174dc2345c18f4f92907cf1e52d5333bbc00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9675 sha256=77a969d412fc87b6afb42b1e3a0d4a0eb943e6bc4e102a6ee08d9574d3a3f3b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/45/ac/c7557ccd8fe79de5da94d7b39b5ca92f22f86a0f85367e2b0a\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=76742b3e5afcbfa565ebbfea9ba759589a9f4436d7801d6e7b1229773e3f2345\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116685 sha256=590ea363c2491947ce5f3313d7d175148ec431f690ff55dc5e125b261ffeecb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n",
            "  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentencepiece: filename=sentencepiece-0.1.95-cp310-cp310-linux_x86_64.whl size=1546196 sha256=c665b059db83591754f412c588e7db8110a9a94f3674e0fc9c3a05dd42cf585c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/a4/01/5a500fc0c5a38917ef408c245eb40b7ac96f4a30fc6a346a4c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=57fe2886388d7e646bc17e543decd785ca60707e5f0ef8fc656917ac23cd2a2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=026da88545b54d523dfcd062dc96e45926346e5e6d1eda1d0bb99b62c69617e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10173 sha256=bcf8e592824fbbe9aeef5847ce20ca5cbd9c7d5ab8819f1014e9962e1d54eabf\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/23/63/4d5849844f8f9d32be09e1b9b278e80de2d8314fbf1e28068b\n",
            "Successfully built fasttext sacremoses gdown googletrans mpld3 sentencepiece sqlitedict langdetect overrides\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, rfc3986, overrides, mpld3, janome, hyperframe, hpack, h11, chardet, tqdm, SoMaJo, segtok, pybind11, mysql-connector-python, more-itertools, langdetect, importlib-metadata, idna, hstspreload, h2, ftfy, deprecated, conllu, cachetools, beautifulsoup4, sacremoses, httpcore, google-auth, gensim, fasttext, wikipedia-api, konoha, huggingface-hub, httpx, google-api-core, deepl, bpemb, transformers, googletrans, google-cloud-core, gdown, google-cloud-translate, BackTranslation, flair\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 9.1.0\n",
            "    Uninstalling more-itertools-9.1.0:\n",
            "      Successfully uninstalled more-itertools-9.1.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.1\n",
            "    Uninstalling gensim-4.3.1:\n",
            "      Successfully uninstalled gensim-4.3.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 2.3.2\n",
            "    Uninstalling google-cloud-core-2.3.2:\n",
            "      Successfully uninstalled google-cloud-core-2.3.2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "  Attempting uninstall: google-cloud-translate\n",
            "    Found existing installation: google-cloud-translate 3.11.1\n",
            "    Uninstalling google-cloud-translate-3.11.1:\n",
            "      Successfully uninstalled google-cloud-translate-3.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
            "google-cloud-storage 2.8.0 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed BackTranslation-0.3.1 SoMaJo-2.2.1 beautifulsoup4-4.11.1 bpemb-0.3.4 cachetools-4.2.4 chardet-3.0.4 conllu-4.5.2 deepl-1.6.0 deprecated-1.2.13 fasttext-0.9.2 flair-0.10 ftfy-6.1.1 gdown-3.12.2 gensim-4.2.0 google-api-core-1.34.0 google-auth-1.35.0 google-cloud-core-1.7.3 google-cloud-translate-2.0.1 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 huggingface-hub-0.14.1 hyperframe-5.2.0 idna-2.10 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 mysql-connector-python-8.0.29 overrides-3.1.0 pybind11-2.10.4 rfc3986-1.5.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.19.1 wikipedia-api-0.5.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaaEJmqsKZ22",
        "outputId": "7b8c20a5-2b66-4d4a-b754-fd90daf0f67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('./src')"
      ],
      "metadata": {
        "id": "n_zC5-oXMROi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python aug/synonym_replace.py datasets/___1.1 0.6 clm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5NC7981MJ5Q",
        "outputId": "39cb8fa4-5200-4321-d42e-895f39f7d6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 03:02:43.311099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 03:02:44.214984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 03:02:48,828 Reading data from datasets/___1.1\n",
            "2023-05-14 03:02:48,828 Train: datasets/___1.1/train.txt\n",
            "2023-05-14 03:02:48,828 Dev: datasets/___1.1/dev.txt\n",
            "2023-05-14 03:02:48,828 Test: datasets/___1.1/test.txt\n",
            "initializing source...\n",
            "Downloading: 100% 616/616 [00:00<00:00, 600kB/s]\n",
            "2023-05-14 03:02:55.815259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:55.927393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:55.927714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:55.938676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:55.938894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:55.939049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:56.178841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:56.179082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:56.179236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-14 03:02:56.179373: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-14 03:02:56.179422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13948 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Downloading: 100% 2.09G/2.09G [00:26<00:00, 83.2MB/s]\n",
            "Downloading: 100% 4.83M/4.83M [00:00<00:00, 42.6MB/s]\n",
            "Downloading: 100% 8.68M/8.68M [00:00<00:00, 51.1MB/s]\n",
            "starting augmentation...\n",
            "100% 5851/5851 [40:49<00:00,  2.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python aug/mention_replace.py datasets/___1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGXMRFa3M62y",
        "outputId": "67effeb7-59c1-42e0-eead-c910cc071ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 03:52:11.980760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 03:52:12.876993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 03:52:16,820 Reading data from datasets/___1.1\n",
            "2023-05-14 03:52:16,820 Train: datasets/___1.1/train.txt\n",
            "2023-05-14 03:52:16,821 Dev: datasets/___1.1/dev.txt\n",
            "2023-05-14 03:52:16,821 Test: datasets/___1.1/test.txt\n",
            "100% 5851/5851 [00:36<00:00, 162.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_bilstm.py datasets/___1.1 datasets/___1.1 0.05 32 gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7852JraZGPj",
        "outputId": "43d48074-6945-49b6-90b3-7aa5b2fd4487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 04:51:01.259996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 04:51:03.082182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 04:51:09,953 Reading data from datasets/___1.1\n",
            "2023-05-14 04:51:09,954 Train: datasets/___1.1/train.txt\n",
            "2023-05-14 04:51:09,954 Dev: datasets/___1.1/dev.txt\n",
            "2023-05-14 04:51:09,954 Test: datasets/___1.1/test.txt\n",
            "2023-05-14 04:51:18,440 Computing label dictionary. Progress:\n",
            "100% 5851/5851 [00:00<00:00, 8295.40it/s]\n",
            "2023-05-14 04:51:19,147 Corpus contains the labels: ner (#325938)\n",
            "2023-05-14 04:51:19,147 Created (for label 'ner') Dictionary with 30 tags: <unk>, O, B-STATUTE, I-STATUTE, B-JUDGE, B-PRECEDENT, I-PRECEDENT, B-PROVISION, I-PROVISION, B-OTHER_PERSON, B-GPE, B-ORG, I-ORG, B-PETITIONER, B-DATE, B-WITNESS, I-WITNESS, B-COURT, I-COURT, B-CASE_NUMBER, I-CASE_NUMBER, I-OTHER_PERSON, B-RESPONDENT, I-RESPONDENT, I-DATE, I-PETITIONER, I-JUDGE, I-GPE, B-LAWYER, I-LAWYER\n",
            "2023-05-14 04:51:19,567 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-news-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpdcovipah\n",
            "100% 1200000128/1200000128 [00:52<00:00, 22851791.65B/s]\n",
            "2023-05-14 04:52:12,600 copying /tmp/tmpdcovipah to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M.vectors.npy\n",
            "2023-05-14 04:52:17,158 removing temp file /tmp/tmpdcovipah\n",
            "2023-05-14 04:52:17,891 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-news-300d-1M not found in cache, downloading to /tmp/tmpuuvarfhf\n",
            "100% 54600983/54600983 [00:03<00:00, 17598420.55B/s]\n",
            "2023-05-14 04:52:21,388 copying /tmp/tmpuuvarfhf to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M\n",
            "2023-05-14 04:52:21,437 removing temp file /tmp/tmpuuvarfhf\n",
            "2023-05-14 04:52:35,434 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmp3uabs7s7\n",
            "100% 73034624/73034624 [00:03<00:00, 18573431.59B/s]\n",
            "2023-05-14 04:52:39,765 copying /tmp/tmp3uabs7s7 to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n",
            "2023-05-14 04:52:39,844 removing temp file /tmp/tmp3uabs7s7\n",
            "2023-05-14 04:52:41,036 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmptv23pyl9\n",
            "100% 73034575/73034575 [00:03<00:00, 18680430.57B/s]\n",
            "2023-05-14 04:52:45,347 copying /tmp/tmptv23pyl9 to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n",
            "2023-05-14 04:52:45,429 removing temp file /tmp/tmptv23pyl9\n",
            "2023-05-14 04:52:46,310 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,310 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings(\n",
            "      'en'\n",
            "      (embedding): Embedding(1000001, 300)\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4396, out_features=4396, bias=True)\n",
            "  (rnn): LSTM(4396, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-05-14 04:52:46,311 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,311 Corpus: \"Corpus: 5851 train + 4109 dev + 982 test sentences\"\n",
            "2023-05-14 04:52:46,311 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,311 Parameters:\n",
            "2023-05-14 04:52:46,311  - learning_rate: \"0.05\"\n",
            "2023-05-14 04:52:46,312  - mini_batch_size: \"32\"\n",
            "2023-05-14 04:52:46,312  - patience: \"5\"\n",
            "2023-05-14 04:52:46,312  - anneal_factor: \"0.5\"\n",
            "2023-05-14 04:52:46,312  - max_epochs: \"10\"\n",
            "2023-05-14 04:52:46,312  - shuffle: \"True\"\n",
            "2023-05-14 04:52:46,312  - train_with_dev: \"False\"\n",
            "2023-05-14 04:52:46,313  - batch_growth_annealing: \"False\"\n",
            "2023-05-14 04:52:46,313 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,313 Model training base path: \"resources/taggers/sota-ner-flair\"\n",
            "2023-05-14 04:52:46,313 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,313 Device: cuda:0\n",
            "2023-05-14 04:52:46,313 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:46,314 Embeddings storage mode: gpu\n",
            "2023-05-14 04:52:46,324 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:52:56,930 epoch 1 - iter 18/183 - loss 1.97771499 - samples/sec: 54.33 - lr: 0.050000\n",
            "2023-05-14 04:53:07,763 epoch 1 - iter 36/183 - loss 1.38736465 - samples/sec: 53.18 - lr: 0.050000\n",
            "2023-05-14 04:53:18,747 epoch 1 - iter 54/183 - loss 1.17508122 - samples/sec: 52.46 - lr: 0.050000\n",
            "2023-05-14 04:53:30,088 epoch 1 - iter 72/183 - loss 1.04079967 - samples/sec: 50.80 - lr: 0.050000\n",
            "2023-05-14 04:53:41,716 epoch 1 - iter 90/183 - loss 0.93434477 - samples/sec: 49.54 - lr: 0.050000\n",
            "2023-05-14 04:53:55,034 epoch 1 - iter 108/183 - loss 0.85917110 - samples/sec: 43.26 - lr: 0.050000\n",
            "2023-05-14 04:54:07,346 epoch 1 - iter 126/183 - loss 0.80669950 - samples/sec: 46.80 - lr: 0.050000\n",
            "2023-05-14 04:55:00,555 epoch 1 - iter 144/183 - loss 0.88325789 - samples/sec: 10.83 - lr: 0.050000\n",
            "2023-05-14 04:56:33,455 epoch 1 - iter 162/183 - loss 0.87567037 - samples/sec: 6.20 - lr: 0.050000\n",
            "2023-05-14 04:58:20,788 epoch 1 - iter 180/183 - loss 0.84976509 - samples/sec: 5.37 - lr: 0.050000\n",
            "2023-05-14 04:58:36,858 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:58:36,859 EPOCH 1 done: loss 0.8498 - lr 0.0500000\n",
            "2023-05-14 05:00:00,256 DEV : loss 0.8125934600830078 - f1-score (micro avg)  0.0385\n",
            "2023-05-14 05:00:00,288 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:00:00,292 saving best model\n",
            "2023-05-14 05:00:10,693 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:00:25,886 epoch 2 - iter 18/183 - loss 0.67394627 - samples/sec: 37.95 - lr: 0.050000\n",
            "2023-05-14 05:00:43,956 epoch 2 - iter 36/183 - loss 0.62353055 - samples/sec: 31.88 - lr: 0.050000\n",
            "2023-05-14 05:00:53,029 epoch 2 - iter 54/183 - loss 0.60659237 - samples/sec: 63.53 - lr: 0.050000\n",
            "2023-05-14 05:01:04,543 epoch 2 - iter 72/183 - loss 0.58519167 - samples/sec: 50.04 - lr: 0.050000\n",
            "2023-05-14 05:01:15,083 epoch 2 - iter 90/183 - loss 0.56370561 - samples/sec: 54.68 - lr: 0.050000\n",
            "2023-05-14 05:01:25,645 epoch 2 - iter 108/183 - loss 0.54546528 - samples/sec: 54.58 - lr: 0.050000\n",
            "2023-05-14 05:01:35,439 epoch 2 - iter 126/183 - loss 0.53781023 - samples/sec: 58.85 - lr: 0.050000\n",
            "2023-05-14 05:01:45,854 epoch 2 - iter 144/183 - loss 0.52427562 - samples/sec: 55.32 - lr: 0.050000\n",
            "2023-05-14 05:01:57,737 epoch 2 - iter 162/183 - loss 0.51071119 - samples/sec: 48.49 - lr: 0.050000\n",
            "2023-05-14 05:02:08,730 epoch 2 - iter 180/183 - loss 0.50162719 - samples/sec: 52.42 - lr: 0.050000\n",
            "2023-05-14 05:02:10,536 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:02:10,537 EPOCH 2 done: loss 0.5004 - lr 0.0500000\n",
            "2023-05-14 05:02:27,876 DEV : loss 0.30097466707229614 - f1-score (micro avg)  0.3564\n",
            "2023-05-14 05:02:27,906 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:02:28,094 saving best model\n",
            "2023-05-14 05:02:35,042 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:02:48,634 epoch 3 - iter 18/183 - loss 0.39956868 - samples/sec: 42.39 - lr: 0.050000\n",
            "2023-05-14 05:03:02,466 epoch 3 - iter 36/183 - loss 0.39762236 - samples/sec: 41.66 - lr: 0.050000\n",
            "2023-05-14 05:03:13,224 epoch 3 - iter 54/183 - loss 0.38669800 - samples/sec: 53.56 - lr: 0.050000\n",
            "2023-05-14 05:03:24,282 epoch 3 - iter 72/183 - loss 0.37614721 - samples/sec: 52.11 - lr: 0.050000\n",
            "2023-05-14 05:03:35,545 epoch 3 - iter 90/183 - loss 0.36801347 - samples/sec: 51.16 - lr: 0.050000\n",
            "2023-05-14 05:03:45,861 epoch 3 - iter 108/183 - loss 0.36449049 - samples/sec: 55.86 - lr: 0.050000\n",
            "2023-05-14 05:03:55,754 epoch 3 - iter 126/183 - loss 0.35526816 - samples/sec: 58.25 - lr: 0.050000\n",
            "2023-05-14 05:04:04,464 epoch 3 - iter 144/183 - loss 0.35176512 - samples/sec: 66.18 - lr: 0.050000\n",
            "2023-05-14 05:04:16,068 epoch 3 - iter 162/183 - loss 0.34650561 - samples/sec: 49.65 - lr: 0.050000\n",
            "2023-05-14 05:04:30,307 epoch 3 - iter 180/183 - loss 0.33914709 - samples/sec: 40.46 - lr: 0.050000\n",
            "2023-05-14 05:04:32,617 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:04:32,621 EPOCH 3 done: loss 0.3383 - lr 0.0500000\n",
            "2023-05-14 05:04:47,946 DEV : loss 0.23881132900714874 - f1-score (micro avg)  0.4707\n",
            "2023-05-14 05:04:47,976 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:04:48,183 saving best model\n",
            "2023-05-14 05:04:55,392 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:05:05,467 epoch 4 - iter 18/183 - loss 0.30243132 - samples/sec: 57.19 - lr: 0.050000\n",
            "2023-05-14 05:05:20,970 epoch 4 - iter 36/183 - loss 0.29568613 - samples/sec: 37.16 - lr: 0.050000\n",
            "2023-05-14 05:05:31,713 epoch 4 - iter 54/183 - loss 0.28925886 - samples/sec: 53.63 - lr: 0.050000\n",
            "2023-05-14 05:05:41,975 epoch 4 - iter 72/183 - loss 0.28519836 - samples/sec: 56.16 - lr: 0.050000\n",
            "2023-05-14 05:05:52,913 epoch 4 - iter 90/183 - loss 0.28677858 - samples/sec: 52.68 - lr: 0.050000\n",
            "2023-05-14 05:06:02,767 epoch 4 - iter 108/183 - loss 0.28115184 - samples/sec: 58.49 - lr: 0.050000\n",
            "2023-05-14 05:06:13,198 epoch 4 - iter 126/183 - loss 0.27713831 - samples/sec: 55.24 - lr: 0.050000\n",
            "2023-05-14 05:06:23,539 epoch 4 - iter 144/183 - loss 0.27424810 - samples/sec: 55.74 - lr: 0.050000\n",
            "2023-05-14 05:06:34,811 epoch 4 - iter 162/183 - loss 0.27360926 - samples/sec: 51.13 - lr: 0.050000\n",
            "2023-05-14 05:06:45,517 epoch 4 - iter 180/183 - loss 0.27001299 - samples/sec: 53.83 - lr: 0.050000\n",
            "2023-05-14 05:06:52,265 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:06:52,266 EPOCH 4 done: loss 0.2774 - lr 0.0500000\n",
            "2023-05-14 05:07:10,517 DEV : loss 0.22944188117980957 - f1-score (micro avg)  0.5136\n",
            "2023-05-14 05:07:10,567 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:07:10,761 saving best model\n",
            "2023-05-14 05:07:19,365 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:07:31,618 epoch 5 - iter 18/183 - loss 0.24629137 - samples/sec: 47.04 - lr: 0.050000\n",
            "2023-05-14 05:07:45,897 epoch 5 - iter 36/183 - loss 0.23771115 - samples/sec: 40.36 - lr: 0.050000\n",
            "2023-05-14 05:07:56,309 epoch 5 - iter 54/183 - loss 0.23545221 - samples/sec: 55.35 - lr: 0.050000\n",
            "2023-05-14 05:08:06,336 epoch 5 - iter 72/183 - loss 0.23530234 - samples/sec: 57.47 - lr: 0.050000\n",
            "2023-05-14 05:08:23,141 epoch 5 - iter 90/183 - loss 0.22781678 - samples/sec: 34.29 - lr: 0.050000\n",
            "2023-05-14 05:08:33,960 epoch 5 - iter 108/183 - loss 0.22913182 - samples/sec: 53.27 - lr: 0.050000\n",
            "2023-05-14 05:08:45,149 epoch 5 - iter 126/183 - loss 0.22771255 - samples/sec: 51.50 - lr: 0.050000\n",
            "2023-05-14 05:08:55,817 epoch 5 - iter 144/183 - loss 0.22674535 - samples/sec: 54.01 - lr: 0.050000\n",
            "2023-05-14 05:09:05,743 epoch 5 - iter 162/183 - loss 0.22556870 - samples/sec: 58.07 - lr: 0.050000\n",
            "2023-05-14 05:09:15,393 epoch 5 - iter 180/183 - loss 0.22381891 - samples/sec: 59.71 - lr: 0.050000\n",
            "2023-05-14 05:09:16,914 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:09:16,915 EPOCH 5 done: loss 0.2234 - lr 0.0500000\n",
            "2023-05-14 05:09:32,008 DEV : loss 0.17692364752292633 - f1-score (micro avg)  0.5967\n",
            "2023-05-14 05:09:32,040 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:09:32,218 saving best model\n",
            "2023-05-14 05:09:39,220 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:09:51,094 epoch 6 - iter 18/183 - loss 0.21004482 - samples/sec: 48.52 - lr: 0.050000\n",
            "2023-05-14 05:10:06,858 epoch 6 - iter 36/183 - loss 0.19919559 - samples/sec: 36.55 - lr: 0.050000\n",
            "2023-05-14 05:10:15,848 epoch 6 - iter 54/183 - loss 0.20215511 - samples/sec: 64.11 - lr: 0.050000\n",
            "2023-05-14 05:10:26,163 epoch 6 - iter 72/183 - loss 0.20075071 - samples/sec: 55.87 - lr: 0.050000\n",
            "2023-05-14 05:10:37,057 epoch 6 - iter 90/183 - loss 0.20407995 - samples/sec: 52.91 - lr: 0.050000\n",
            "2023-05-14 05:10:48,889 epoch 6 - iter 108/183 - loss 0.20269238 - samples/sec: 48.70 - lr: 0.050000\n",
            "2023-05-14 05:10:58,135 epoch 6 - iter 126/183 - loss 0.19950185 - samples/sec: 62.32 - lr: 0.050000\n",
            "2023-05-14 05:11:13,829 epoch 6 - iter 144/183 - loss 0.19575586 - samples/sec: 36.72 - lr: 0.050000\n",
            "2023-05-14 05:11:24,767 epoch 6 - iter 162/183 - loss 0.19470105 - samples/sec: 52.68 - lr: 0.050000\n",
            "2023-05-14 05:11:36,261 epoch 6 - iter 180/183 - loss 0.19287122 - samples/sec: 50.13 - lr: 0.050000\n",
            "2023-05-14 05:11:37,868 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:11:37,871 EPOCH 6 done: loss 0.1932 - lr 0.0500000\n",
            "2023-05-14 05:11:55,017 DEV : loss 0.1714039295911789 - f1-score (micro avg)  0.6156\n",
            "2023-05-14 05:11:55,049 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:11:55,253 saving best model\n",
            "2023-05-14 05:12:07,348 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:12:20,681 epoch 7 - iter 18/183 - loss 0.18125896 - samples/sec: 43.21 - lr: 0.050000\n",
            "2023-05-14 05:12:34,651 epoch 7 - iter 36/183 - loss 0.18067756 - samples/sec: 41.25 - lr: 0.050000\n",
            "2023-05-14 05:12:43,433 epoch 7 - iter 54/183 - loss 0.18757719 - samples/sec: 65.63 - lr: 0.050000\n",
            "2023-05-14 05:12:58,688 epoch 7 - iter 72/183 - loss 0.18007326 - samples/sec: 37.77 - lr: 0.050000\n",
            "2023-05-14 05:13:10,778 epoch 7 - iter 90/183 - loss 0.17764067 - samples/sec: 47.66 - lr: 0.050000\n",
            "2023-05-14 05:13:22,764 epoch 7 - iter 108/183 - loss 0.17951643 - samples/sec: 48.09 - lr: 0.050000\n",
            "2023-05-14 05:13:32,095 epoch 7 - iter 126/183 - loss 0.17732517 - samples/sec: 61.76 - lr: 0.050000\n",
            "2023-05-14 05:13:41,176 epoch 7 - iter 144/183 - loss 0.17572981 - samples/sec: 63.45 - lr: 0.050000\n",
            "2023-05-14 05:13:51,359 epoch 7 - iter 162/183 - loss 0.17538131 - samples/sec: 56.58 - lr: 0.050000\n",
            "2023-05-14 05:14:04,831 epoch 7 - iter 180/183 - loss 0.17354497 - samples/sec: 42.78 - lr: 0.050000\n",
            "2023-05-14 05:14:06,384 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:14:06,387 EPOCH 7 done: loss 0.1731 - lr 0.0500000\n",
            "2023-05-14 05:14:23,725 DEV : loss 0.14552414417266846 - f1-score (micro avg)  0.6555\n",
            "2023-05-14 05:14:23,757 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:14:23,761 saving best model\n",
            "2023-05-14 05:14:30,610 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:14:52,702 epoch 8 - iter 18/183 - loss 0.15610483 - samples/sec: 26.08 - lr: 0.050000\n",
            "2023-05-14 05:15:03,949 epoch 8 - iter 36/183 - loss 0.15554650 - samples/sec: 51.24 - lr: 0.050000\n",
            "2023-05-14 05:15:14,682 epoch 8 - iter 54/183 - loss 0.15905295 - samples/sec: 53.70 - lr: 0.050000\n",
            "2023-05-14 05:15:23,931 epoch 8 - iter 72/183 - loss 0.15731683 - samples/sec: 62.30 - lr: 0.050000\n",
            "2023-05-14 05:15:35,577 epoch 8 - iter 90/183 - loss 0.15819608 - samples/sec: 49.47 - lr: 0.050000\n",
            "2023-05-14 05:15:44,921 epoch 8 - iter 108/183 - loss 0.15746385 - samples/sec: 61.67 - lr: 0.050000\n",
            "2023-05-14 05:15:54,588 epoch 8 - iter 126/183 - loss 0.15730683 - samples/sec: 59.62 - lr: 0.050000\n",
            "2023-05-14 05:16:05,001 epoch 8 - iter 144/183 - loss 0.15675206 - samples/sec: 55.33 - lr: 0.050000\n",
            "2023-05-14 05:16:14,645 epoch 8 - iter 162/183 - loss 0.15689025 - samples/sec: 59.74 - lr: 0.050000\n",
            "2023-05-14 05:16:25,161 epoch 8 - iter 180/183 - loss 0.15575264 - samples/sec: 54.79 - lr: 0.050000\n",
            "2023-05-14 05:16:27,127 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:16:27,128 EPOCH 8 done: loss 0.1555 - lr 0.0500000\n",
            "2023-05-14 05:16:42,306 DEV : loss 0.13706225156784058 - f1-score (micro avg)  0.6727\n",
            "2023-05-14 05:16:42,337 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:16:42,559 saving best model\n",
            "2023-05-14 05:16:49,836 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:17:02,372 epoch 9 - iter 18/183 - loss 0.15995075 - samples/sec: 45.99 - lr: 0.050000\n",
            "2023-05-14 05:17:17,584 epoch 9 - iter 36/183 - loss 0.15055762 - samples/sec: 37.89 - lr: 0.050000\n",
            "2023-05-14 05:17:33,990 epoch 9 - iter 54/183 - loss 0.14377218 - samples/sec: 35.12 - lr: 0.050000\n",
            "2023-05-14 05:17:43,769 epoch 9 - iter 72/183 - loss 0.14273329 - samples/sec: 58.92 - lr: 0.050000\n",
            "2023-05-14 05:17:54,093 epoch 9 - iter 90/183 - loss 0.14394182 - samples/sec: 55.81 - lr: 0.050000\n",
            "2023-05-14 05:18:02,461 epoch 9 - iter 108/183 - loss 0.14652139 - samples/sec: 68.85 - lr: 0.050000\n",
            "2023-05-14 05:18:13,260 epoch 9 - iter 126/183 - loss 0.14440405 - samples/sec: 53.36 - lr: 0.050000\n",
            "2023-05-14 05:18:24,660 epoch 9 - iter 144/183 - loss 0.14272484 - samples/sec: 50.55 - lr: 0.050000\n",
            "2023-05-14 05:18:33,871 epoch 9 - iter 162/183 - loss 0.14381458 - samples/sec: 62.56 - lr: 0.050000\n",
            "2023-05-14 05:18:45,967 epoch 9 - iter 180/183 - loss 0.14409547 - samples/sec: 47.64 - lr: 0.050000\n",
            "2023-05-14 05:18:47,814 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:18:47,819 EPOCH 9 done: loss 0.1439 - lr 0.0500000\n",
            "2023-05-14 05:19:06,721 DEV : loss 0.12432385236024857 - f1-score (micro avg)  0.6911\n",
            "2023-05-14 05:19:06,777 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:19:07,027 saving best model\n",
            "2023-05-14 05:19:14,641 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:19:25,606 epoch 10 - iter 18/183 - loss 0.13429616 - samples/sec: 52.55 - lr: 0.050000\n",
            "2023-05-14 05:19:43,536 epoch 10 - iter 36/183 - loss 0.13128792 - samples/sec: 32.14 - lr: 0.050000\n",
            "2023-05-14 05:19:55,898 epoch 10 - iter 54/183 - loss 0.13473232 - samples/sec: 46.61 - lr: 0.050000\n",
            "2023-05-14 05:20:06,317 epoch 10 - iter 72/183 - loss 0.13409631 - samples/sec: 55.31 - lr: 0.050000\n",
            "2023-05-14 05:20:16,915 epoch 10 - iter 90/183 - loss 0.13550060 - samples/sec: 54.37 - lr: 0.050000\n",
            "2023-05-14 05:20:29,632 epoch 10 - iter 108/183 - loss 0.13374581 - samples/sec: 45.31 - lr: 0.050000\n",
            "2023-05-14 05:20:39,422 epoch 10 - iter 126/183 - loss 0.13284453 - samples/sec: 58.87 - lr: 0.050000\n",
            "2023-05-14 05:20:48,934 epoch 10 - iter 144/183 - loss 0.13233130 - samples/sec: 60.58 - lr: 0.050000\n",
            "2023-05-14 05:20:58,592 epoch 10 - iter 162/183 - loss 0.13461260 - samples/sec: 59.66 - lr: 0.050000\n",
            "2023-05-14 05:21:10,360 epoch 10 - iter 180/183 - loss 0.13363611 - samples/sec: 48.96 - lr: 0.050000\n",
            "2023-05-14 05:21:11,929 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:21:11,931 EPOCH 10 done: loss 0.1338 - lr 0.0500000\n",
            "2023-05-14 05:21:27,104 DEV : loss 0.11429645121097565 - f1-score (micro avg)  0.7064\n",
            "2023-05-14 05:21:27,134 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:21:27,388 saving best model\n",
            "2023-05-14 05:21:48,900 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:21:48,950 loading file resources/taggers/sota-ner-flair/best-model.pt\n",
            "2023-05-14 05:22:45,344 0.838\t0.7663\t0.8006\t0.6944\n",
            "2023-05-14 05:22:45,345 \n",
            "Results:\n",
            "- F-score (micro) 0.8006\n",
            "- F-score (macro) 0.7347\n",
            "- Accuracy 0.6944\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      LAWYER     0.9605    0.9083    0.9337       589\n",
            "  RESPONDENT     0.7940    0.7587    0.7760       315\n",
            "       COURT     0.8839    0.7973    0.8384       296\n",
            "OTHER_PERSON     0.7882    0.7283    0.7571       276\n",
            "   PROVISION     0.8720    0.8450    0.8583       258\n",
            "     STATUTE     0.8532    0.8378    0.8455       222\n",
            "        DATE     0.9535    0.9234    0.9382       222\n",
            "  PETITIONER     0.8119    0.7773    0.7942       211\n",
            "   PRECEDENT     0.6593    0.6780    0.6685       177\n",
            "         GPE     0.7186    0.6557    0.6857       183\n",
            "       JUDGE     0.8428    0.7701    0.8048       174\n",
            "         ORG     0.6437    0.3522    0.4553       159\n",
            " CASE_NUMBER     0.7212    0.6198    0.6667       121\n",
            "     WITNESS     0.5556    0.1724    0.2632        58\n",
            "\n",
            "   micro avg     0.8380    0.7663    0.8006      3261\n",
            "   macro avg     0.7899    0.7017    0.7347      3261\n",
            "weighted avg     0.8308    0.7663    0.7938      3261\n",
            " samples avg     0.6944    0.6944    0.6944      3261\n",
            "\n",
            "2023-05-14 05:22:45,345 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:22:45,370 loading file resources/taggers/sota-ner-flair/final-model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_bilstm.py datasets/cSR1.0f0.6pclm  datasets/cSR1.0f0.6pclm  0.05 32 gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F7ijgb2bh-j",
        "outputId": "636f290c-fa59-4886-ff0d-75de270b992d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 05:29:19.951719: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 05:29:21.776524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 05:29:29,567 Reading data from datasets/cSR1.0f0.6pclm\n",
            "2023-05-14 05:29:29,567 Train: datasets/cSR1.0f0.6pclm/train.txt\n",
            "2023-05-14 05:29:29,567 Dev: datasets/cSR1.0f0.6pclm/dev.txt\n",
            "2023-05-14 05:29:29,567 Test: datasets/cSR1.0f0.6pclm/test.txt\n",
            "2023-05-14 05:29:36,213 Computing label dictionary. Progress:\n",
            "100% 6031/6031 [00:00<00:00, 12304.61it/s]\n",
            "2023-05-14 05:29:36,704 Corpus contains the labels: ner (#330596)\n",
            "2023-05-14 05:29:36,705 Created (for label 'ner') Dictionary with 30 tags: <unk>, O, B-STATUTE, I-STATUTE, B-JUDGE, B-PRECEDENT, I-PRECEDENT, B-PROVISION, I-PROVISION, B-OTHER_PERSON, B-GPE, B-ORG, I-ORG, B-PETITIONER, B-DATE, B-WITNESS, I-WITNESS, B-COURT, I-COURT, B-CASE_NUMBER, I-CASE_NUMBER, I-OTHER_PERSON, B-RESPONDENT, I-RESPONDENT, I-DATE, I-PETITIONER, I-JUDGE, I-GPE, B-LAWYER, I-LAWYER\n",
            "2023-05-14 05:29:54,255 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,256 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings(\n",
            "      'en'\n",
            "      (embedding): Embedding(1000001, 300)\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4396, out_features=4396, bias=True)\n",
            "  (rnn): LSTM(4396, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-05-14 05:29:54,256 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,256 Corpus: \"Corpus: 6031 train + 4109 dev + 982 test sentences\"\n",
            "2023-05-14 05:29:54,257 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,257 Parameters:\n",
            "2023-05-14 05:29:54,257  - learning_rate: \"0.05\"\n",
            "2023-05-14 05:29:54,257  - mini_batch_size: \"32\"\n",
            "2023-05-14 05:29:54,257  - patience: \"5\"\n",
            "2023-05-14 05:29:54,258  - anneal_factor: \"0.5\"\n",
            "2023-05-14 05:29:54,258  - max_epochs: \"10\"\n",
            "2023-05-14 05:29:54,258  - shuffle: \"True\"\n",
            "2023-05-14 05:29:54,258  - train_with_dev: \"False\"\n",
            "2023-05-14 05:29:54,258  - batch_growth_annealing: \"False\"\n",
            "2023-05-14 05:29:54,259 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,259 Model training base path: \"resources/taggers/sota-ner-flair\"\n",
            "2023-05-14 05:29:54,259 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,259 Device: cuda:0\n",
            "2023-05-14 05:29:54,259 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:29:54,260 Embeddings storage mode: gpu\n",
            "/usr/local/lib/python3.10/dist-packages/flair/trainers/trainer.py:64: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  warnings.warn(\n",
            "2023-05-14 05:29:54,541 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:30:05,618 epoch 1 - iter 18/189 - loss 1.64510956 - samples/sec: 52.01 - lr: 0.050000\n",
            "2023-05-14 05:30:16,283 epoch 1 - iter 36/189 - loss 1.25052818 - samples/sec: 54.02 - lr: 0.050000\n",
            "2023-05-14 05:30:26,647 epoch 1 - iter 54/189 - loss 1.10490591 - samples/sec: 55.59 - lr: 0.050000\n",
            "2023-05-14 05:30:37,374 epoch 1 - iter 72/189 - loss 0.99803876 - samples/sec: 53.71 - lr: 0.050000\n",
            "2023-05-14 05:30:49,986 epoch 1 - iter 90/189 - loss 0.90851954 - samples/sec: 45.68 - lr: 0.050000\n",
            "2023-05-14 05:31:01,313 epoch 1 - iter 108/189 - loss 0.84536144 - samples/sec: 50.87 - lr: 0.050000\n",
            "2023-05-14 05:31:12,789 epoch 1 - iter 126/189 - loss 0.80157120 - samples/sec: 50.20 - lr: 0.050000\n",
            "2023-05-14 05:32:04,026 epoch 1 - iter 144/189 - loss 0.84372204 - samples/sec: 11.24 - lr: 0.050000\n",
            "2023-05-14 05:33:37,303 epoch 1 - iter 162/189 - loss 0.80800946 - samples/sec: 6.18 - lr: 0.050000\n",
            "2023-05-14 05:35:27,157 epoch 1 - iter 180/189 - loss 0.77113044 - samples/sec: 5.24 - lr: 0.050000\n",
            "2023-05-14 05:35:44,798 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:35:44,799 EPOCH 1 done: loss 0.7675 - lr 0.0500000\n",
            "2023-05-14 05:37:07,965 DEV : loss 0.45620015263557434 - f1-score (micro avg)  0.2609\n",
            "2023-05-14 05:37:07,995 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:37:08,371 saving best model\n",
            "2023-05-14 05:37:36,015 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:37:56,680 epoch 2 - iter 18/189 - loss 0.52677682 - samples/sec: 27.89 - lr: 0.050000\n",
            "2023-05-14 05:38:08,785 epoch 2 - iter 36/189 - loss 0.53053603 - samples/sec: 47.61 - lr: 0.050000\n",
            "2023-05-14 05:38:20,055 epoch 2 - iter 54/189 - loss 0.52789005 - samples/sec: 51.14 - lr: 0.050000\n",
            "2023-05-14 05:38:29,557 epoch 2 - iter 72/189 - loss 0.51288211 - samples/sec: 60.64 - lr: 0.050000\n",
            "2023-05-14 05:38:38,521 epoch 2 - iter 90/189 - loss 0.50492593 - samples/sec: 64.30 - lr: 0.050000\n",
            "2023-05-14 05:38:47,772 epoch 2 - iter 108/189 - loss 0.49541972 - samples/sec: 62.29 - lr: 0.050000\n",
            "2023-05-14 05:38:58,361 epoch 2 - iter 126/189 - loss 0.48636567 - samples/sec: 54.42 - lr: 0.050000\n",
            "2023-05-14 05:39:09,524 epoch 2 - iter 144/189 - loss 0.47901932 - samples/sec: 51.63 - lr: 0.050000\n",
            "2023-05-14 05:39:18,906 epoch 2 - iter 162/189 - loss 0.46877052 - samples/sec: 61.42 - lr: 0.050000\n",
            "2023-05-14 05:39:30,768 epoch 2 - iter 180/189 - loss 0.45870228 - samples/sec: 48.57 - lr: 0.050000\n",
            "2023-05-14 05:39:35,665 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:39:35,668 EPOCH 2 done: loss 0.4541 - lr 0.0500000\n",
            "2023-05-14 05:39:52,387 DEV : loss 0.3248496949672699 - f1-score (micro avg)  0.3649\n",
            "2023-05-14 05:39:52,416 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:39:52,802 saving best model\n",
            "2023-05-14 05:40:20,864 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:40:33,252 epoch 3 - iter 18/189 - loss 0.37463993 - samples/sec: 46.51 - lr: 0.050000\n",
            "2023-05-14 05:40:53,479 epoch 3 - iter 36/189 - loss 0.34534329 - samples/sec: 28.48 - lr: 0.050000\n",
            "2023-05-14 05:41:02,306 epoch 3 - iter 54/189 - loss 0.33979702 - samples/sec: 65.29 - lr: 0.050000\n",
            "2023-05-14 05:41:11,865 epoch 3 - iter 72/189 - loss 0.33990372 - samples/sec: 60.28 - lr: 0.050000\n",
            "2023-05-14 05:41:23,714 epoch 3 - iter 90/189 - loss 0.34090701 - samples/sec: 48.64 - lr: 0.050000\n",
            "2023-05-14 05:41:33,123 epoch 3 - iter 108/189 - loss 0.33413329 - samples/sec: 61.24 - lr: 0.050000\n",
            "2023-05-14 05:41:44,724 epoch 3 - iter 126/189 - loss 0.33141668 - samples/sec: 49.68 - lr: 0.050000\n",
            "2023-05-14 05:41:54,897 epoch 3 - iter 144/189 - loss 0.32933069 - samples/sec: 56.66 - lr: 0.050000\n",
            "2023-05-14 05:42:06,773 epoch 3 - iter 162/189 - loss 0.32642180 - samples/sec: 48.52 - lr: 0.050000\n",
            "2023-05-14 05:42:15,643 epoch 3 - iter 180/189 - loss 0.32300786 - samples/sec: 64.98 - lr: 0.050000\n",
            "2023-05-14 05:42:20,774 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:42:20,775 EPOCH 3 done: loss 0.3217 - lr 0.0500000\n",
            "2023-05-14 05:42:35,357 DEV : loss 0.2490919679403305 - f1-score (micro avg)  0.4741\n",
            "2023-05-14 05:42:35,407 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:42:35,793 saving best model\n",
            "2023-05-14 05:42:57,376 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:43:11,364 epoch 4 - iter 18/189 - loss 0.26097630 - samples/sec: 41.19 - lr: 0.050000\n",
            "2023-05-14 05:43:25,951 epoch 4 - iter 36/189 - loss 0.26626773 - samples/sec: 39.50 - lr: 0.050000\n",
            "2023-05-14 05:43:35,605 epoch 4 - iter 54/189 - loss 0.27292014 - samples/sec: 59.69 - lr: 0.050000\n",
            "2023-05-14 05:43:45,971 epoch 4 - iter 72/189 - loss 0.26918521 - samples/sec: 55.59 - lr: 0.050000\n",
            "2023-05-14 05:43:56,374 epoch 4 - iter 90/189 - loss 0.27384550 - samples/sec: 55.39 - lr: 0.050000\n",
            "2023-05-14 05:44:06,987 epoch 4 - iter 108/189 - loss 0.27123215 - samples/sec: 54.30 - lr: 0.050000\n",
            "2023-05-14 05:44:16,809 epoch 4 - iter 126/189 - loss 0.26599746 - samples/sec: 58.68 - lr: 0.050000\n",
            "2023-05-14 05:44:27,189 epoch 4 - iter 144/189 - loss 0.26367668 - samples/sec: 55.52 - lr: 0.050000\n",
            "2023-05-14 05:44:43,004 epoch 4 - iter 162/189 - loss 0.25834161 - samples/sec: 36.43 - lr: 0.050000\n",
            "2023-05-14 05:44:55,004 epoch 4 - iter 180/189 - loss 0.25486827 - samples/sec: 48.01 - lr: 0.050000\n",
            "2023-05-14 05:44:59,119 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:44:59,121 EPOCH 4 done: loss 0.2554 - lr 0.0500000\n",
            "2023-05-14 05:45:16,444 DEV : loss 0.21162086725234985 - f1-score (micro avg)  0.5479\n",
            "2023-05-14 05:45:16,473 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:45:16,851 saving best model\n",
            "2023-05-14 05:45:44,911 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:45:57,959 epoch 5 - iter 18/189 - loss 0.21117801 - samples/sec: 44.16 - lr: 0.050000\n",
            "2023-05-14 05:46:08,351 epoch 5 - iter 36/189 - loss 0.21686292 - samples/sec: 55.45 - lr: 0.050000\n",
            "2023-05-14 05:46:18,366 epoch 5 - iter 54/189 - loss 0.22260896 - samples/sec: 57.55 - lr: 0.050000\n",
            "2023-05-14 05:46:35,321 epoch 5 - iter 72/189 - loss 0.21659859 - samples/sec: 33.99 - lr: 0.050000\n",
            "2023-05-14 05:46:45,604 epoch 5 - iter 90/189 - loss 0.21712119 - samples/sec: 56.05 - lr: 0.050000\n",
            "2023-05-14 05:46:57,971 epoch 5 - iter 108/189 - loss 0.21959140 - samples/sec: 46.59 - lr: 0.050000\n",
            "2023-05-14 05:47:07,174 epoch 5 - iter 126/189 - loss 0.21914564 - samples/sec: 62.62 - lr: 0.050000\n",
            "2023-05-14 05:47:18,900 epoch 5 - iter 144/189 - loss 0.21549331 - samples/sec: 49.13 - lr: 0.050000\n",
            "2023-05-14 05:47:28,827 epoch 5 - iter 162/189 - loss 0.21397293 - samples/sec: 58.04 - lr: 0.050000\n",
            "2023-05-14 05:47:37,675 epoch 5 - iter 180/189 - loss 0.21402725 - samples/sec: 65.14 - lr: 0.050000\n",
            "2023-05-14 05:47:42,905 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:47:42,905 EPOCH 5 done: loss 0.2137 - lr 0.0500000\n",
            "2023-05-14 05:47:57,799 DEV : loss 0.17843399941921234 - f1-score (micro avg)  0.61\n",
            "2023-05-14 05:47:57,828 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:47:57,832 saving best model\n",
            "2023-05-14 05:48:04,879 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:19,085 epoch 6 - iter 18/189 - loss 0.18563880 - samples/sec: 40.56 - lr: 0.050000\n",
            "2023-05-14 05:48:32,835 epoch 6 - iter 36/189 - loss 0.19864246 - samples/sec: 41.91 - lr: 0.050000\n",
            "2023-05-14 05:48:42,849 epoch 6 - iter 54/189 - loss 0.19684206 - samples/sec: 57.54 - lr: 0.050000\n",
            "2023-05-14 05:48:52,283 epoch 6 - iter 72/189 - loss 0.19740743 - samples/sec: 61.08 - lr: 0.050000\n",
            "2023-05-14 05:49:04,123 epoch 6 - iter 90/189 - loss 0.19493262 - samples/sec: 48.66 - lr: 0.050000\n",
            "2023-05-14 05:49:14,662 epoch 6 - iter 108/189 - loss 0.19288839 - samples/sec: 54.68 - lr: 0.050000\n",
            "2023-05-14 05:49:23,394 epoch 6 - iter 126/189 - loss 0.19253808 - samples/sec: 65.99 - lr: 0.050000\n",
            "2023-05-14 05:49:33,529 epoch 6 - iter 144/189 - loss 0.19143038 - samples/sec: 56.85 - lr: 0.050000\n",
            "2023-05-14 05:49:51,727 epoch 6 - iter 162/189 - loss 0.19494521 - samples/sec: 31.66 - lr: 0.050000\n",
            "2023-05-14 05:50:01,615 epoch 6 - iter 180/189 - loss 0.19368876 - samples/sec: 58.27 - lr: 0.050000\n",
            "2023-05-14 05:50:05,492 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:50:05,493 EPOCH 6 done: loss 0.1933 - lr 0.0500000\n",
            "2023-05-14 05:50:22,287 DEV : loss 0.1613156795501709 - f1-score (micro avg)  0.6304\n",
            "2023-05-14 05:50:22,317 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:50:22,321 saving best model\n",
            "2023-05-14 05:50:30,685 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:50:42,242 epoch 7 - iter 18/189 - loss 0.18427530 - samples/sec: 49.85 - lr: 0.050000\n",
            "2023-05-14 05:50:55,146 epoch 7 - iter 36/189 - loss 0.17555377 - samples/sec: 44.68 - lr: 0.050000\n",
            "2023-05-14 05:51:06,751 epoch 7 - iter 54/189 - loss 0.17151712 - samples/sec: 49.65 - lr: 0.050000\n",
            "2023-05-14 05:51:18,220 epoch 7 - iter 72/189 - loss 0.17789109 - samples/sec: 50.25 - lr: 0.050000\n",
            "2023-05-14 05:51:31,545 epoch 7 - iter 90/189 - loss 0.17255155 - samples/sec: 43.24 - lr: 0.050000\n",
            "2023-05-14 05:51:44,017 epoch 7 - iter 108/189 - loss 0.17355752 - samples/sec: 46.20 - lr: 0.050000\n",
            "2023-05-14 05:51:53,314 epoch 7 - iter 126/189 - loss 0.17252285 - samples/sec: 61.99 - lr: 0.050000\n",
            "2023-05-14 05:52:03,341 epoch 7 - iter 144/189 - loss 0.17185819 - samples/sec: 57.46 - lr: 0.050000\n",
            "2023-05-14 05:52:14,970 epoch 7 - iter 162/189 - loss 0.17126905 - samples/sec: 49.54 - lr: 0.050000\n",
            "2023-05-14 05:52:24,263 epoch 7 - iter 180/189 - loss 0.17023011 - samples/sec: 62.01 - lr: 0.050000\n",
            "2023-05-14 05:52:30,320 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:52:30,320 EPOCH 7 done: loss 0.1712 - lr 0.0500000\n",
            "2023-05-14 05:52:46,835 DEV : loss 0.14832352101802826 - f1-score (micro avg)  0.6468\n",
            "2023-05-14 05:52:46,863 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:52:46,866 saving best model\n",
            "2023-05-14 05:52:56,040 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:53:05,864 epoch 8 - iter 18/189 - loss 0.16000205 - samples/sec: 58.68 - lr: 0.050000\n",
            "2023-05-14 05:53:22,374 epoch 8 - iter 36/189 - loss 0.16072052 - samples/sec: 34.90 - lr: 0.050000\n",
            "2023-05-14 05:53:34,466 epoch 8 - iter 54/189 - loss 0.15819541 - samples/sec: 47.65 - lr: 0.050000\n",
            "2023-05-14 05:53:50,786 epoch 8 - iter 72/189 - loss 0.15419340 - samples/sec: 35.30 - lr: 0.050000\n",
            "2023-05-14 05:54:01,549 epoch 8 - iter 90/189 - loss 0.15527329 - samples/sec: 53.55 - lr: 0.050000\n",
            "2023-05-14 05:54:11,305 epoch 8 - iter 108/189 - loss 0.15441522 - samples/sec: 59.06 - lr: 0.050000\n",
            "2023-05-14 05:54:21,868 epoch 8 - iter 126/189 - loss 0.15469564 - samples/sec: 54.56 - lr: 0.050000\n",
            "2023-05-14 05:54:32,411 epoch 8 - iter 144/189 - loss 0.15548732 - samples/sec: 54.66 - lr: 0.050000\n",
            "2023-05-14 05:54:43,921 epoch 8 - iter 162/189 - loss 0.15424421 - samples/sec: 50.06 - lr: 0.050000\n",
            "2023-05-14 05:54:54,044 epoch 8 - iter 180/189 - loss 0.15455080 - samples/sec: 56.91 - lr: 0.050000\n",
            "2023-05-14 05:54:58,170 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:54:58,173 EPOCH 8 done: loss 0.1547 - lr 0.0500000\n",
            "2023-05-14 05:55:13,066 DEV : loss 0.12766985595226288 - f1-score (micro avg)  0.6784\n",
            "2023-05-14 05:55:13,120 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:55:13,123 saving best model\n",
            "2023-05-14 05:55:24,595 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:55:39,244 epoch 9 - iter 18/189 - loss 0.16413836 - samples/sec: 39.33 - lr: 0.050000\n",
            "2023-05-14 05:55:53,376 epoch 9 - iter 36/189 - loss 0.15179262 - samples/sec: 40.78 - lr: 0.050000\n",
            "2023-05-14 05:56:03,787 epoch 9 - iter 54/189 - loss 0.14836555 - samples/sec: 55.35 - lr: 0.050000\n",
            "2023-05-14 05:56:13,857 epoch 9 - iter 72/189 - loss 0.14567741 - samples/sec: 57.23 - lr: 0.050000\n",
            "2023-05-14 05:56:24,176 epoch 9 - iter 90/189 - loss 0.14458128 - samples/sec: 55.86 - lr: 0.050000\n",
            "2023-05-14 05:56:40,677 epoch 9 - iter 108/189 - loss 0.14167199 - samples/sec: 34.93 - lr: 0.050000\n",
            "2023-05-14 05:56:50,833 epoch 9 - iter 126/189 - loss 0.14120752 - samples/sec: 56.75 - lr: 0.050000\n",
            "2023-05-14 05:57:00,172 epoch 9 - iter 144/189 - loss 0.14044669 - samples/sec: 61.71 - lr: 0.050000\n",
            "2023-05-14 05:57:11,182 epoch 9 - iter 162/189 - loss 0.14124939 - samples/sec: 52.33 - lr: 0.050000\n",
            "2023-05-14 05:57:22,225 epoch 9 - iter 180/189 - loss 0.14059542 - samples/sec: 52.19 - lr: 0.050000\n",
            "2023-05-14 05:57:26,440 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:57:26,440 EPOCH 9 done: loss 0.1399 - lr 0.0500000\n",
            "2023-05-14 05:57:43,933 DEV : loss 0.12200852483510971 - f1-score (micro avg)  0.687\n",
            "2023-05-14 05:57:43,985 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:57:44,003 saving best model\n",
            "2023-05-14 05:57:54,905 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:58:16,282 epoch 10 - iter 18/189 - loss 0.11255304 - samples/sec: 26.95 - lr: 0.050000\n",
            "2023-05-14 05:58:26,929 epoch 10 - iter 36/189 - loss 0.11989789 - samples/sec: 54.12 - lr: 0.050000\n",
            "2023-05-14 05:58:38,236 epoch 10 - iter 54/189 - loss 0.12344387 - samples/sec: 50.95 - lr: 0.050000\n",
            "2023-05-14 05:58:47,757 epoch 10 - iter 72/189 - loss 0.12697591 - samples/sec: 60.53 - lr: 0.050000\n",
            "2023-05-14 05:58:59,423 epoch 10 - iter 90/189 - loss 0.12596404 - samples/sec: 49.40 - lr: 0.050000\n",
            "2023-05-14 05:59:09,759 epoch 10 - iter 108/189 - loss 0.12737351 - samples/sec: 55.76 - lr: 0.050000\n",
            "2023-05-14 05:59:19,396 epoch 10 - iter 126/189 - loss 0.12890150 - samples/sec: 59.80 - lr: 0.050000\n",
            "2023-05-14 05:59:28,911 epoch 10 - iter 144/189 - loss 0.13023865 - samples/sec: 60.57 - lr: 0.050000\n",
            "2023-05-14 05:59:39,434 epoch 10 - iter 162/189 - loss 0.13130060 - samples/sec: 54.75 - lr: 0.050000\n",
            "2023-05-14 05:59:49,321 epoch 10 - iter 180/189 - loss 0.13137970 - samples/sec: 58.29 - lr: 0.050000\n",
            "2023-05-14 05:59:53,745 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:59:53,746 EPOCH 10 done: loss 0.1319 - lr 0.0500000\n",
            "2023-05-14 06:00:08,451 DEV : loss 0.11445135623216629 - f1-score (micro avg)  0.7018\n",
            "2023-05-14 06:00:08,482 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 06:00:08,485 saving best model\n",
            "2023-05-14 06:00:29,310 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 06:00:29,399 loading file resources/taggers/sota-ner-flair/best-model.pt\n",
            "2023-05-14 06:01:25,975 0.8412\t0.7568\t0.7968\t0.6894\n",
            "2023-05-14 06:01:25,975 \n",
            "Results:\n",
            "- F-score (micro) 0.7968\n",
            "- F-score (macro) 0.7138\n",
            "- Accuracy 0.6894\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      LAWYER     0.9669    0.8930    0.9285       589\n",
            "  RESPONDENT     0.7695    0.7841    0.7767       315\n",
            "OTHER_PERSON     0.7322    0.7826    0.7566       276\n",
            "       COURT     0.8893    0.7872    0.8351       296\n",
            "   PROVISION     0.9080    0.8798    0.8937       258\n",
            "     STATUTE     0.8520    0.8559    0.8539       222\n",
            "        DATE     0.9491    0.9234    0.9361       222\n",
            "  PETITIONER     0.9177    0.6872    0.7859       211\n",
            "   PRECEDENT     0.6766    0.6384    0.6570       177\n",
            "       JUDGE     0.8973    0.7529    0.8187       174\n",
            "         GPE     0.7969    0.5574    0.6559       183\n",
            "         ORG     0.4924    0.4088    0.4467       159\n",
            " CASE_NUMBER     0.7640    0.5620    0.6476       121\n",
            "     WITNESS     0.0000    0.0000    0.0000        58\n",
            "\n",
            "   micro avg     0.8412    0.7568    0.7968      3261\n",
            "   macro avg     0.7580    0.6795    0.7138      3261\n",
            "weighted avg     0.8272    0.7568    0.7880      3261\n",
            " samples avg     0.6894    0.6894    0.6894      3261\n",
            "\n",
            "2023-05-14 06:01:25,975 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 06:01:26,010 loading file resources/taggers/sota-ner-flair/final-model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyFiatrwvAbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}