{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_azskjZemQm",
        "outputId": "e25c910b-df7b-4c75-956a-d47a7d33bded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/data-augmentation-ner-legal-main/\")"
      ],
      "metadata": {
        "id": "iM7hFaejetLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zRGaJP_Ae3DJ",
        "outputId": "864c3705-2075-4463-de55-b6dbfc005c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair==0.10 (from -r requirements.txt (line 1))\n",
            "  Using cached flair-0.10-py3-none-any.whl (322 kB)\n",
            "Collecting BackTranslation==0.3.1 (from -r requirements.txt (line 2))\n",
            "  Using cached BackTranslation-0.3.1-py3-none-any.whl (8.9 kB)\n",
            "Collecting beautifulsoup4==4.11.1 (from -r requirements.txt (line 3))\n",
            "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "Collecting deepl==1.6.0 (from -r requirements.txt (line 4))\n",
            "  Using cached deepl-1.6.0-py3-none-any.whl (31 kB)\n",
            "Collecting fasttext==0.9.2 (from -r requirements.txt (line 5))\n",
            "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim==4.2.0 (from -r requirements.txt (line 6))\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mysql-connector-python==8.0.29 (from -r requirements.txt (line 7))\n",
            "  Downloading mysql_connector_python-8.0.29-cp310-cp310-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.0.53 (from -r requirements.txt (line 8))\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting SoMaJo==2.2.1 (from -r requirements.txt (line 9))\n",
            "  Downloading SoMaJo-2.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from -r requirements.txt (line 10))\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.19.1 (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-translate==2.0.1 (from -r requirements.txt (line 12))\n",
            "  Downloading google_cloud_translate-2.0.1-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Collecting segtok>=1.5.7 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (3.7.1)\n",
            "Collecting mpld3==0.3 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (1.2.2)\n",
            "Collecting sqlitedict>=1.6.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting bpemb>=0.3.2 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (0.8.10)\n",
            "Collecting langdetect (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair==0.10->-r requirements.txt (line 1)) (4.9.2)\n",
            "Collecting ftfy (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.95 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading sentencepiece-0.1.95.tar.gz (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konoha<5.0.0,>=4.0.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting janome (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gdown==3.12.2 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting more-itertools~=8.8.0 (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api (from flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Collecting googletrans==4.0.0rc1 (from BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from BackTranslation==0.3.1->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.11.1->-r requirements.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl==1.6.0->-r requirements.txt (line 4)) (2.27.1)\n",
            "Collecting pybind11>=2.2 (from fasttext==0.9.2->-r requirements.txt (line 5))\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext==0.9.2->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->-r requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->-r requirements.txt (line 6)) (6.3.0)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mysql-connector-python==8.0.29->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 11)) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.1->-r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.15.0 (from google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1->BackTranslation==0.3.1->-r requirements.txt (line 2))\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair==0.10->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (1.48.2)\n",
            "Collecting google-auth<3.0dev,>=1.25.0 (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->flair==0.10->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->flair==0.10->-r requirements.txt (line 1)) (4.5.0)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0 (from konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0 (from konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1))\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair==0.10->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair==0.10->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (16.0.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair==0.10->-r requirements.txt (line 1)) (0.2.6)\n",
            "Collecting cachetools<5.0,>=2.0.0 (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12))\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10->-r requirements.txt (line 1)) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl==1.6.0->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair==0.10->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1->-r requirements.txt (line 12)) (0.5.0)\n",
            "Building wheels for collected packages: fasttext, sacremoses, gdown, googletrans, mpld3, sentencepiece, sqlitedict, langdetect, overrides\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393173 sha256=2b9231a4976f97670d1a0123edb2095d3a8dcc40d8411fdb1aae115b249c3e57\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=8492108d83ab37aec62b7c3edcd7a51bec91fd17ef8cea8409754f2cf1d3f3a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9675 sha256=6f4dabf1bc367b93435f66e7c8b951cd1f4769012cac04ff3f9e565cb4f639ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/45/ac/c7557ccd8fe79de5da94d7b39b5ca92f22f86a0f85367e2b0a\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=d395979cc25dc78fcb46f0a004bf9f9c02a9421fb3051687de3748bb430dfb86\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116685 sha256=e38c6df1eddb1d895c2da2054a3682e870f204b203958e6b8e0ae2254728a96f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n",
            "  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentencepiece: filename=sentencepiece-0.1.95-cp310-cp310-linux_x86_64.whl size=1546217 sha256=3a466ed36a12e10c59c27b8cf663b798f8122e712786b8d01b49a067b003784b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/a4/01/5a500fc0c5a38917ef408c245eb40b7ac96f4a30fc6a346a4c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=d6c1dd8ee848027da295f60064ac951fb7443fa62642df43f0e675a694ecd215\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=67016ac3de7f2e8a3e1ee560bd9a7e0464b4d9c30779654658fc05ea772a4eea\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10173 sha256=ae31fc3b03556b4c25eaca2370218f48800bdf7283ca20a2d50e9bb1b10d725f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/23/63/4d5849844f8f9d32be09e1b9b278e80de2d8314fbf1e28068b\n",
            "Successfully built fasttext sacremoses gdown googletrans mpld3 sentencepiece sqlitedict langdetect overrides\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, rfc3986, overrides, mpld3, janome, hyperframe, hpack, h11, chardet, tqdm, SoMaJo, segtok, pybind11, mysql-connector-python, more-itertools, langdetect, importlib-metadata, idna, hstspreload, h2, ftfy, deprecated, conllu, cachetools, beautifulsoup4, sacremoses, httpcore, google-auth, gensim, fasttext, wikipedia-api, konoha, huggingface-hub, httpx, google-api-core, deepl, bpemb, transformers, googletrans, google-cloud-core, gdown, google-cloud-translate, BackTranslation, flair\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 9.1.0\n",
            "    Uninstalling more-itertools-9.1.0:\n",
            "      Successfully uninstalled more-itertools-9.1.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.1\n",
            "    Uninstalling gensim-4.3.1:\n",
            "      Successfully uninstalled gensim-4.3.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 2.3.2\n",
            "    Uninstalling google-cloud-core-2.3.2:\n",
            "      Successfully uninstalled google-cloud-core-2.3.2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "  Attempting uninstall: google-cloud-translate\n",
            "    Found existing installation: google-cloud-translate 3.11.1\n",
            "    Uninstalling google-cloud-translate-3.11.1:\n",
            "      Successfully uninstalled google-cloud-translate-3.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-auth-oauthlib 1.0.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
            "google-cloud-storage 2.8.0 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed BackTranslation-0.3.1 SoMaJo-2.2.1 beautifulsoup4-4.11.1 bpemb-0.3.4 cachetools-4.2.4 chardet-3.0.4 conllu-4.5.2 deepl-1.6.0 deprecated-1.2.13 fasttext-0.9.2 flair-0.10 ftfy-6.1.1 gdown-3.12.2 gensim-4.2.0 google-api-core-1.34.0 google-auth-1.35.0 google-cloud-core-1.7.3 google-cloud-translate-2.0.1 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 huggingface-hub-0.14.1 hyperframe-5.2.0 idna-2.10 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 mysql-connector-python-8.0.29 overrides-3.1.0 pybind11-2.10.4 rfc3986-1.5.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.19.1 wikipedia-api-0.5.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0tz6qSue3t7",
        "outputId": "aa6e5c4b-da18-4121-9915-7f8ef5a3a68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('./src')"
      ],
      "metadata": {
        "id": "reZDpLUffRmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_bilstm.py datasets/cMR1.0  datasets/cMR1.0  0.05 32 gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIGe_uXge7MD",
        "outputId": "6fb8bb67-4c88-464f-db02-a78f6ced094b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 04:51:43.417953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 04:51:44.745302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 04:51:49,085 Reading data from datasets/cMR1.0\n",
            "2023-05-14 04:51:49,085 Train: datasets/cMR1.0/train.txt\n",
            "2023-05-14 04:51:49,085 Dev: datasets/cMR1.0/dev.txt\n",
            "2023-05-14 04:51:49,085 Test: datasets/cMR1.0/test.txt\n",
            "2023-05-14 04:51:55,966 Computing label dictionary. Progress:\n",
            "100% 6025/6025 [00:00<00:00, 12373.60it/s]\n",
            "2023-05-14 04:51:56,454 Corpus contains the labels: ner (#331168)\n",
            "2023-05-14 04:51:56,454 Created (for label 'ner') Dictionary with 30 tags: <unk>, O, B-STATUTE, I-STATUTE, B-JUDGE, B-PRECEDENT, I-PRECEDENT, B-PROVISION, I-PROVISION, B-OTHER_PERSON, B-GPE, B-ORG, I-ORG, B-PETITIONER, B-DATE, B-WITNESS, I-WITNESS, B-COURT, I-COURT, B-CASE_NUMBER, I-CASE_NUMBER, I-OTHER_PERSON, B-RESPONDENT, I-RESPONDENT, I-DATE, I-PETITIONER, I-JUDGE, I-GPE, B-LAWYER, I-LAWYER\n",
            "2023-05-14 04:51:57,139 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-news-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpxabm1a10\n",
            "100% 1200000128/1200000128 [00:53<00:00, 22495012.22B/s]\n",
            "2023-05-14 04:52:50,887 copying /tmp/tmpxabm1a10 to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M.vectors.npy\n",
            "2023-05-14 04:52:54,969 removing temp file /tmp/tmpxabm1a10\n",
            "2023-05-14 04:52:55,770 https://flair.informatik.hu-berlin.de/resources/embeddings/token/en-fasttext-news-300d-1M not found in cache, downloading to /tmp/tmplp11m37k\n",
            "100% 54600983/54600983 [00:03<00:00, 17099651.69B/s]\n",
            "2023-05-14 04:52:59,366 copying /tmp/tmplp11m37k to cache at /root/.flair/embeddings/en-fasttext-news-300d-1M\n",
            "2023-05-14 04:52:59,437 removing temp file /tmp/tmplp11m37k\n",
            "2023-05-14 04:53:12,419 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpp75ht3s1\n",
            "100% 73034624/73034624 [00:03<00:00, 18347952.52B/s]\n",
            "2023-05-14 04:53:16,804 copying /tmp/tmpp75ht3s1 to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n",
            "2023-05-14 04:53:16,884 removing temp file /tmp/tmpp75ht3s1\n",
            "2023-05-14 04:53:17,951 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp6rf6t40h\n",
            "100% 73034575/73034575 [00:03<00:00, 18478911.92B/s]\n",
            "2023-05-14 04:53:22,299 copying /tmp/tmp6rf6t40h to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n",
            "2023-05-14 04:53:22,393 removing temp file /tmp/tmp6rf6t40h\n",
            "2023-05-14 04:53:22,920 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,921 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings(\n",
            "      'en'\n",
            "      (embedding): Embedding(1000001, 300)\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4396, out_features=4396, bias=True)\n",
            "  (rnn): LSTM(4396, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-05-14 04:53:22,921 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,921 Corpus: \"Corpus: 6025 train + 4109 dev + 982 test sentences\"\n",
            "2023-05-14 04:53:22,922 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,922 Parameters:\n",
            "2023-05-14 04:53:22,922  - learning_rate: \"0.05\"\n",
            "2023-05-14 04:53:22,922  - mini_batch_size: \"32\"\n",
            "2023-05-14 04:53:22,922  - patience: \"5\"\n",
            "2023-05-14 04:53:22,922  - anneal_factor: \"0.5\"\n",
            "2023-05-14 04:53:22,922  - max_epochs: \"10\"\n",
            "2023-05-14 04:53:22,923  - shuffle: \"True\"\n",
            "2023-05-14 04:53:22,923  - train_with_dev: \"False\"\n",
            "2023-05-14 04:53:22,923  - batch_growth_annealing: \"False\"\n",
            "2023-05-14 04:53:22,923 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,923 Model training base path: \"resources/taggers/sota-ner-flair\"\n",
            "2023-05-14 04:53:22,923 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,924 Device: cuda:0\n",
            "2023-05-14 04:53:22,924 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:22,924 Embeddings storage mode: gpu\n",
            "2023-05-14 04:53:22,932 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:53:33,839 epoch 1 - iter 18/189 - loss 1.69764988 - samples/sec: 52.83 - lr: 0.050000\n",
            "2023-05-14 04:53:44,363 epoch 1 - iter 36/189 - loss 1.26488907 - samples/sec: 54.74 - lr: 0.050000\n",
            "2023-05-14 04:53:54,579 epoch 1 - iter 54/189 - loss 1.10590738 - samples/sec: 56.40 - lr: 0.050000\n",
            "2023-05-14 04:54:05,312 epoch 1 - iter 72/189 - loss 0.99278098 - samples/sec: 53.68 - lr: 0.050000\n",
            "2023-05-14 04:54:18,146 epoch 1 - iter 90/189 - loss 0.90232642 - samples/sec: 44.89 - lr: 0.050000\n",
            "2023-05-14 04:54:29,009 epoch 1 - iter 108/189 - loss 0.83619669 - samples/sec: 53.04 - lr: 0.050000\n",
            "2023-05-14 04:54:40,299 epoch 1 - iter 126/189 - loss 0.79225404 - samples/sec: 51.03 - lr: 0.050000\n",
            "2023-05-14 04:55:28,294 epoch 1 - iter 144/189 - loss 0.86001713 - samples/sec: 12.00 - lr: 0.050000\n",
            "2023-05-14 04:56:52,768 epoch 1 - iter 162/189 - loss 0.84566989 - samples/sec: 6.82 - lr: 0.050000\n",
            "2023-05-14 04:58:31,983 epoch 1 - iter 180/189 - loss 0.81412053 - samples/sec: 5.81 - lr: 0.050000\n",
            "2023-05-14 04:58:47,947 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 04:58:47,947 EPOCH 1 done: loss 0.8132 - lr 0.0500000\n",
            "2023-05-14 05:00:04,189 DEV : loss 0.47545933723449707 - f1-score (micro avg)  0.2179\n",
            "2023-05-14 05:00:04,238 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:00:04,243 saving best model\n",
            "2023-05-14 05:00:13,762 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:00:26,478 epoch 2 - iter 18/189 - loss 0.62472556 - samples/sec: 45.31 - lr: 0.050000\n",
            "2023-05-14 05:00:40,448 epoch 2 - iter 36/189 - loss 0.61163740 - samples/sec: 41.25 - lr: 0.050000\n",
            "2023-05-14 05:00:49,802 epoch 2 - iter 54/189 - loss 0.59432490 - samples/sec: 61.61 - lr: 0.050000\n",
            "2023-05-14 05:01:00,245 epoch 2 - iter 72/189 - loss 0.58459491 - samples/sec: 55.17 - lr: 0.050000\n",
            "2023-05-14 05:01:12,171 epoch 2 - iter 90/189 - loss 0.56806875 - samples/sec: 48.31 - lr: 0.050000\n",
            "2023-05-14 05:01:29,078 epoch 2 - iter 108/189 - loss 0.54495947 - samples/sec: 34.08 - lr: 0.050000\n",
            "2023-05-14 05:01:38,377 epoch 2 - iter 126/189 - loss 0.53398581 - samples/sec: 61.97 - lr: 0.050000\n",
            "2023-05-14 05:01:48,227 epoch 2 - iter 144/189 - loss 0.51961464 - samples/sec: 58.51 - lr: 0.050000\n",
            "2023-05-14 05:01:58,641 epoch 2 - iter 162/189 - loss 0.50799449 - samples/sec: 55.33 - lr: 0.050000\n",
            "2023-05-14 05:02:08,526 epoch 2 - iter 180/189 - loss 0.49835618 - samples/sec: 58.30 - lr: 0.050000\n",
            "2023-05-14 05:02:14,282 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:02:14,282 EPOCH 2 done: loss 0.4937 - lr 0.0500000\n",
            "2023-05-14 05:02:30,778 DEV : loss 0.399173378944397 - f1-score (micro avg)  0.2869\n",
            "2023-05-14 05:02:30,807 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:02:30,809 saving best model\n",
            "2023-05-14 05:02:37,757 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:02:49,757 epoch 3 - iter 18/189 - loss 0.38493509 - samples/sec: 48.01 - lr: 0.050000\n",
            "2023-05-14 05:03:07,228 epoch 3 - iter 36/189 - loss 0.36020368 - samples/sec: 32.98 - lr: 0.050000\n",
            "2023-05-14 05:03:20,008 epoch 3 - iter 54/189 - loss 0.36341345 - samples/sec: 45.09 - lr: 0.050000\n",
            "2023-05-14 05:03:31,051 epoch 3 - iter 72/189 - loss 0.36046633 - samples/sec: 52.18 - lr: 0.050000\n",
            "2023-05-14 05:03:43,064 epoch 3 - iter 90/189 - loss 0.35555599 - samples/sec: 47.97 - lr: 0.050000\n",
            "2023-05-14 05:03:52,837 epoch 3 - iter 108/189 - loss 0.35529305 - samples/sec: 58.98 - lr: 0.050000\n",
            "2023-05-14 05:04:03,580 epoch 3 - iter 126/189 - loss 0.35284965 - samples/sec: 53.66 - lr: 0.050000\n",
            "2023-05-14 05:04:12,999 epoch 3 - iter 144/189 - loss 0.34717356 - samples/sec: 61.20 - lr: 0.050000\n",
            "2023-05-14 05:04:22,348 epoch 3 - iter 162/189 - loss 0.34380954 - samples/sec: 61.64 - lr: 0.050000\n",
            "2023-05-14 05:04:32,438 epoch 3 - iter 180/189 - loss 0.33911560 - samples/sec: 57.12 - lr: 0.050000\n",
            "2023-05-14 05:04:36,122 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:04:36,122 EPOCH 3 done: loss 0.3378 - lr 0.0500000\n",
            "2023-05-14 05:04:51,298 DEV : loss 0.2575187385082245 - f1-score (micro avg)  0.5241\n",
            "2023-05-14 05:04:51,328 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:04:51,331 saving best model\n",
            "2023-05-14 05:04:58,262 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:05:08,173 epoch 4 - iter 18/189 - loss 0.29072420 - samples/sec: 58.14 - lr: 0.050000\n",
            "2023-05-14 05:05:21,258 epoch 4 - iter 36/189 - loss 0.29609596 - samples/sec: 44.06 - lr: 0.050000\n",
            "2023-05-14 05:05:32,095 epoch 4 - iter 54/189 - loss 0.28827771 - samples/sec: 53.18 - lr: 0.050000\n",
            "2023-05-14 05:05:42,019 epoch 4 - iter 72/189 - loss 0.28679039 - samples/sec: 58.06 - lr: 0.050000\n",
            "2023-05-14 05:05:53,457 epoch 4 - iter 90/189 - loss 0.28509247 - samples/sec: 50.37 - lr: 0.050000\n",
            "2023-05-14 05:06:07,364 epoch 4 - iter 108/189 - loss 0.27977223 - samples/sec: 41.44 - lr: 0.050000\n",
            "2023-05-14 05:06:18,720 epoch 4 - iter 126/189 - loss 0.27621705 - samples/sec: 50.73 - lr: 0.050000\n",
            "2023-05-14 05:06:28,308 epoch 4 - iter 144/189 - loss 0.27368765 - samples/sec: 60.12 - lr: 0.050000\n",
            "2023-05-14 05:06:37,881 epoch 4 - iter 162/189 - loss 0.27316778 - samples/sec: 60.21 - lr: 0.050000\n",
            "2023-05-14 05:06:49,346 epoch 4 - iter 180/189 - loss 0.26857542 - samples/sec: 50.27 - lr: 0.050000\n",
            "2023-05-14 05:06:54,690 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:06:54,691 EPOCH 4 done: loss 0.2665 - lr 0.0500000\n",
            "2023-05-14 05:07:13,420 DEV : loss 0.21379521489143372 - f1-score (micro avg)  0.5778\n",
            "2023-05-14 05:07:13,469 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:07:13,709 saving best model\n",
            "2023-05-14 05:07:21,220 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:07:35,448 epoch 5 - iter 18/189 - loss 0.24706901 - samples/sec: 40.50 - lr: 0.050000\n",
            "2023-05-14 05:07:49,789 epoch 5 - iter 36/189 - loss 0.24104732 - samples/sec: 40.19 - lr: 0.050000\n",
            "2023-05-14 05:07:59,567 epoch 5 - iter 54/189 - loss 0.23809966 - samples/sec: 58.93 - lr: 0.050000\n",
            "2023-05-14 05:08:10,908 epoch 5 - iter 72/189 - loss 0.23076152 - samples/sec: 50.81 - lr: 0.050000\n",
            "2023-05-14 05:08:22,378 epoch 5 - iter 90/189 - loss 0.23199851 - samples/sec: 50.25 - lr: 0.050000\n",
            "2023-05-14 05:08:32,908 epoch 5 - iter 108/189 - loss 0.23091751 - samples/sec: 54.72 - lr: 0.050000\n",
            "2023-05-14 05:08:41,024 epoch 5 - iter 126/189 - loss 0.22842966 - samples/sec: 71.01 - lr: 0.050000\n",
            "2023-05-14 05:08:50,864 epoch 5 - iter 144/189 - loss 0.22640707 - samples/sec: 58.58 - lr: 0.050000\n",
            "2023-05-14 05:09:05,074 epoch 5 - iter 162/189 - loss 0.22551228 - samples/sec: 40.55 - lr: 0.050000\n",
            "2023-05-14 05:09:15,207 epoch 5 - iter 180/189 - loss 0.22281854 - samples/sec: 56.87 - lr: 0.050000\n",
            "2023-05-14 05:09:20,904 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:09:20,908 EPOCH 5 done: loss 0.2223 - lr 0.0500000\n",
            "2023-05-14 05:09:35,944 DEV : loss 0.18317413330078125 - f1-score (micro avg)  0.6041\n",
            "2023-05-14 05:09:35,973 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:09:35,978 saving best model\n",
            "2023-05-14 05:09:42,944 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:09:59,749 epoch 6 - iter 18/189 - loss 0.19737462 - samples/sec: 34.29 - lr: 0.050000\n",
            "2023-05-14 05:10:10,464 epoch 6 - iter 36/189 - loss 0.20023289 - samples/sec: 53.79 - lr: 0.050000\n",
            "2023-05-14 05:10:21,683 epoch 6 - iter 54/189 - loss 0.19863642 - samples/sec: 51.37 - lr: 0.050000\n",
            "2023-05-14 05:10:31,660 epoch 6 - iter 72/189 - loss 0.19832760 - samples/sec: 57.75 - lr: 0.050000\n",
            "2023-05-14 05:10:40,325 epoch 6 - iter 90/189 - loss 0.19728720 - samples/sec: 66.50 - lr: 0.050000\n",
            "2023-05-14 05:10:51,178 epoch 6 - iter 108/189 - loss 0.19829787 - samples/sec: 53.10 - lr: 0.050000\n",
            "2023-05-14 05:10:59,485 epoch 6 - iter 126/189 - loss 0.19873927 - samples/sec: 69.40 - lr: 0.050000\n",
            "2023-05-14 05:11:10,147 epoch 6 - iter 144/189 - loss 0.19800210 - samples/sec: 54.04 - lr: 0.050000\n",
            "2023-05-14 05:11:21,076 epoch 6 - iter 162/189 - loss 0.19689637 - samples/sec: 52.72 - lr: 0.050000\n",
            "2023-05-14 05:11:30,024 epoch 6 - iter 180/189 - loss 0.19668383 - samples/sec: 64.42 - lr: 0.050000\n",
            "2023-05-14 05:11:40,637 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:11:40,638 EPOCH 6 done: loss 0.1947 - lr 0.0500000\n",
            "2023-05-14 05:11:58,129 DEV : loss 0.17184476554393768 - f1-score (micro avg)  0.6313\n",
            "2023-05-14 05:11:58,159 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:11:58,162 saving best model\n",
            "2023-05-14 05:12:12,710 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:12:27,556 epoch 7 - iter 18/189 - loss 0.18017911 - samples/sec: 38.81 - lr: 0.050000\n",
            "2023-05-14 05:12:45,649 epoch 7 - iter 36/189 - loss 0.17522628 - samples/sec: 31.85 - lr: 0.050000\n",
            "2023-05-14 05:12:56,408 epoch 7 - iter 54/189 - loss 0.17504450 - samples/sec: 53.55 - lr: 0.050000\n",
            "2023-05-14 05:13:06,542 epoch 7 - iter 72/189 - loss 0.17258142 - samples/sec: 56.87 - lr: 0.050000\n",
            "2023-05-14 05:13:16,047 epoch 7 - iter 90/189 - loss 0.17433203 - samples/sec: 60.63 - lr: 0.050000\n",
            "2023-05-14 05:13:27,209 epoch 7 - iter 108/189 - loss 0.17463973 - samples/sec: 51.63 - lr: 0.050000\n",
            "2023-05-14 05:13:38,852 epoch 7 - iter 126/189 - loss 0.17525935 - samples/sec: 49.50 - lr: 0.050000\n",
            "2023-05-14 05:13:48,324 epoch 7 - iter 144/189 - loss 0.17389984 - samples/sec: 60.84 - lr: 0.050000\n",
            "2023-05-14 05:13:59,829 epoch 7 - iter 162/189 - loss 0.17244782 - samples/sec: 50.08 - lr: 0.050000\n",
            "2023-05-14 05:14:09,069 epoch 7 - iter 180/189 - loss 0.17445135 - samples/sec: 62.36 - lr: 0.050000\n",
            "2023-05-14 05:14:13,702 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:14:13,706 EPOCH 7 done: loss 0.1743 - lr 0.0500000\n",
            "2023-05-14 05:14:31,598 DEV : loss 0.14201270043849945 - f1-score (micro avg)  0.6607\n",
            "2023-05-14 05:14:31,648 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:14:31,653 saving best model\n",
            "2023-05-14 05:14:39,782 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:14:52,193 epoch 8 - iter 18/189 - loss 0.16237963 - samples/sec: 46.42 - lr: 0.050000\n",
            "2023-05-14 05:15:05,824 epoch 8 - iter 36/189 - loss 0.16145349 - samples/sec: 42.28 - lr: 0.050000\n",
            "2023-05-14 05:15:21,877 epoch 8 - iter 54/189 - loss 0.15652660 - samples/sec: 35.90 - lr: 0.050000\n",
            "2023-05-14 05:15:30,943 epoch 8 - iter 72/189 - loss 0.16052456 - samples/sec: 63.56 - lr: 0.050000\n",
            "2023-05-14 05:15:42,573 epoch 8 - iter 90/189 - loss 0.15920823 - samples/sec: 49.55 - lr: 0.050000\n",
            "2023-05-14 05:15:52,692 epoch 8 - iter 108/189 - loss 0.16147198 - samples/sec: 56.96 - lr: 0.050000\n",
            "2023-05-14 05:16:02,588 epoch 8 - iter 126/189 - loss 0.16375751 - samples/sec: 58.24 - lr: 0.050000\n",
            "2023-05-14 05:16:14,239 epoch 8 - iter 144/189 - loss 0.16289601 - samples/sec: 49.45 - lr: 0.050000\n",
            "2023-05-14 05:16:24,622 epoch 8 - iter 162/189 - loss 0.16140945 - samples/sec: 55.51 - lr: 0.050000\n",
            "2023-05-14 05:16:33,788 epoch 8 - iter 180/189 - loss 0.16101555 - samples/sec: 62.87 - lr: 0.050000\n",
            "2023-05-14 05:16:39,382 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:16:39,382 EPOCH 8 done: loss 0.1617 - lr 0.0500000\n",
            "2023-05-14 05:16:54,391 DEV : loss 0.1378375142812729 - f1-score (micro avg)  0.6731\n",
            "2023-05-14 05:16:54,422 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:16:54,584 saving best model\n",
            "2023-05-14 05:17:04,316 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:17:18,337 epoch 9 - iter 18/189 - loss 0.14805370 - samples/sec: 41.09 - lr: 0.050000\n",
            "2023-05-14 05:17:35,464 epoch 9 - iter 36/189 - loss 0.14529246 - samples/sec: 33.65 - lr: 0.050000\n",
            "2023-05-14 05:17:46,966 epoch 9 - iter 54/189 - loss 0.14259873 - samples/sec: 50.11 - lr: 0.050000\n",
            "2023-05-14 05:17:56,835 epoch 9 - iter 72/189 - loss 0.14038171 - samples/sec: 58.39 - lr: 0.050000\n",
            "2023-05-14 05:18:06,805 epoch 9 - iter 90/189 - loss 0.14161416 - samples/sec: 57.81 - lr: 0.050000\n",
            "2023-05-14 05:18:16,687 epoch 9 - iter 108/189 - loss 0.14499419 - samples/sec: 58.32 - lr: 0.050000\n",
            "2023-05-14 05:18:27,407 epoch 9 - iter 126/189 - loss 0.14560088 - samples/sec: 53.76 - lr: 0.050000\n",
            "2023-05-14 05:18:37,006 epoch 9 - iter 144/189 - loss 0.14595045 - samples/sec: 60.04 - lr: 0.050000\n",
            "2023-05-14 05:18:46,716 epoch 9 - iter 162/189 - loss 0.14601911 - samples/sec: 59.35 - lr: 0.050000\n",
            "2023-05-14 05:18:57,289 epoch 9 - iter 180/189 - loss 0.14632826 - samples/sec: 54.51 - lr: 0.050000\n",
            "2023-05-14 05:19:02,502 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:19:02,503 EPOCH 9 done: loss 0.1461 - lr 0.0500000\n",
            "2023-05-14 05:19:20,311 DEV : loss 0.1373487114906311 - f1-score (micro avg)  0.6655\n",
            "2023-05-14 05:19:20,359 BAD EPOCHS (no improvement): 1\n",
            "2023-05-14 05:19:20,538 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:19:30,492 epoch 10 - iter 18/189 - loss 0.13126582 - samples/sec: 57.89 - lr: 0.050000\n",
            "2023-05-14 05:19:41,549 epoch 10 - iter 36/189 - loss 0.13786977 - samples/sec: 52.11 - lr: 0.050000\n",
            "2023-05-14 05:19:57,256 epoch 10 - iter 54/189 - loss 0.13683913 - samples/sec: 36.69 - lr: 0.050000\n",
            "2023-05-14 05:20:07,841 epoch 10 - iter 72/189 - loss 0.13342439 - samples/sec: 54.44 - lr: 0.050000\n",
            "2023-05-14 05:20:17,290 epoch 10 - iter 90/189 - loss 0.13401496 - samples/sec: 61.00 - lr: 0.050000\n",
            "2023-05-14 05:20:28,135 epoch 10 - iter 108/189 - loss 0.13386974 - samples/sec: 53.15 - lr: 0.050000\n",
            "2023-05-14 05:20:37,643 epoch 10 - iter 126/189 - loss 0.13412813 - samples/sec: 60.60 - lr: 0.050000\n",
            "2023-05-14 05:20:48,383 epoch 10 - iter 144/189 - loss 0.13524198 - samples/sec: 53.65 - lr: 0.050000\n",
            "2023-05-14 05:20:59,216 epoch 10 - iter 162/189 - loss 0.13517763 - samples/sec: 53.19 - lr: 0.050000\n",
            "2023-05-14 05:21:10,296 epoch 10 - iter 180/189 - loss 0.13330748 - samples/sec: 52.00 - lr: 0.050000\n",
            "2023-05-14 05:21:14,282 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:21:14,282 EPOCH 10 done: loss 0.1327 - lr 0.0500000\n",
            "2023-05-14 05:21:29,786 DEV : loss 0.11369308829307556 - f1-score (micro avg)  0.7047\n",
            "2023-05-14 05:21:29,816 BAD EPOCHS (no improvement): 0\n",
            "2023-05-14 05:21:30,197 saving best model\n",
            "2023-05-14 05:21:49,720 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:21:49,775 loading file resources/taggers/sota-ner-flair/best-model.pt\n",
            "2023-05-14 05:22:40,946 0.8146\t0.7909\t0.8026\t0.7006\n",
            "2023-05-14 05:22:40,946 \n",
            "Results:\n",
            "- F-score (micro) 0.8026\n",
            "- F-score (macro) 0.7247\n",
            "- Accuracy 0.7006\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      LAWYER     0.9412    0.9508    0.9459       589\n",
            "  RESPONDENT     0.7326    0.8000    0.7648       315\n",
            "OTHER_PERSON     0.7314    0.8188    0.7726       276\n",
            "       COURT     0.8577    0.7939    0.8246       296\n",
            "   PROVISION     0.8810    0.8605    0.8706       258\n",
            "     STATUTE     0.8000    0.8649    0.8312       222\n",
            "        DATE     0.9364    0.9279    0.9321       222\n",
            "  PETITIONER     0.8229    0.7488    0.7841       211\n",
            "   PRECEDENT     0.6420    0.6384    0.6402       177\n",
            "       JUDGE     0.8457    0.7874    0.8155       174\n",
            "         GPE     0.7582    0.6339    0.6905       183\n",
            "         ORG     0.6015    0.5031    0.5479       159\n",
            " CASE_NUMBER     0.7168    0.6694    0.6923       121\n",
            "     WITNESS     0.3333    0.0172    0.0328        58\n",
            "\n",
            "   micro avg     0.8146    0.7909    0.8026      3261\n",
            "   macro avg     0.7572    0.7154    0.7247      3261\n",
            "weighted avg     0.8060    0.7909    0.7946      3261\n",
            " samples avg     0.7006    0.7006    0.7006      3261\n",
            "\n",
            "2023-05-14 05:22:40,946 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:22:40,956 loading file resources/taggers/sota-ner-flair/final-model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_flert.py datasets/___1.0  datasets/___1.0  1 10 False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwlT3wNNgOFV",
        "outputId": "b46a2950-9c48-4212-ee96-4e94e1f764b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-14 05:47:22.223377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-14 05:47:23.801004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-14 05:47:28,575 Reading data from datasets/___1.0\n",
            "2023-05-14 05:47:28,575 Train: datasets/___1.0/train.txt\n",
            "2023-05-14 05:47:28,575 Dev: datasets/___1.0/dev.txt\n",
            "2023-05-14 05:47:28,575 Test: datasets/___1.0/test.txt\n",
            "2023-05-14 05:47:29,841 Computing label dictionary. Progress:\n",
            "100% 11/11 [00:00<00:00, 8216.80it/s]\n",
            "2023-05-14 05:47:29,844 Corpus contains the labels: ner (#167)\n",
            "2023-05-14 05:47:29,844 Created (for label 'ner') Dictionary with 116 tags: <unk>, On, was, CW3, are, pillion, if, all, Chandregowda, is, had, the, deceased, this, view, Vinay, other, torch, Article, in, Supreme, that, this,, finishing, contention, 2015.07.04, causing, it, United, proved, complete, now, see, to, Superintendent, affirm, dealing, assuming,, might, Arbitrators, asserted, (1), by, will, lays, be, we, Shri., paras, they\n",
            "Downloading: 100% 616/616 [00:00<00:00, 676kB/s]\n",
            "Downloading: 100% 4.83M/4.83M [00:00<00:00, 25.7MB/s]\n",
            "Downloading: 100% 8.68M/8.68M [00:00<00:00, 40.1MB/s]\n",
            "Downloading: 100% 2.09G/2.09G [00:27<00:00, 83.1MB/s]\n",
            "2023-05-14 05:48:09,414 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,418 Model: \"SequenceTagger(\n",
            "  (embeddings): TransformerWordEmbeddings(\n",
            "    (model): XLMRobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 1024)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): RobertaEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-23): 24 x RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): RobertaPooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (linear): Linear(in_features=1024, out_features=116, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-05-14 05:48:09,418 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,418 Corpus: \"Corpus: 11 train + 3 dev + 2 test sentences\"\n",
            "2023-05-14 05:48:09,419 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,419 Parameters:\n",
            "2023-05-14 05:48:09,419  - learning_rate: \"5e-06\"\n",
            "2023-05-14 05:48:09,419  - mini_batch_size: \"1\"\n",
            "2023-05-14 05:48:09,419  - patience: \"3\"\n",
            "2023-05-14 05:48:09,420  - anneal_factor: \"0.5\"\n",
            "2023-05-14 05:48:09,420  - max_epochs: \"10\"\n",
            "2023-05-14 05:48:09,420  - shuffle: \"True\"\n",
            "2023-05-14 05:48:09,420  - train_with_dev: \"False\"\n",
            "2023-05-14 05:48:09,420  - batch_growth_annealing: \"False\"\n",
            "2023-05-14 05:48:09,420 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,421 Model training base path: \"resources/taggers/sota-ner-flert-xlm\"\n",
            "2023-05-14 05:48:09,421 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,421 Device: cuda:0\n",
            "2023-05-14 05:48:09,421 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:09,421 Embeddings storage mode: none\n",
            "2023-05-14 05:48:09,431 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:10,127 epoch 1 - iter 1/11 - loss 4.34532928 - samples/sec: 1.45 - lr: 0.000000\n",
            "2023-05-14 05:48:10,277 epoch 1 - iter 2/11 - loss 4.80993467 - samples/sec: 6.72 - lr: 0.000001\n",
            "2023-05-14 05:48:10,573 epoch 1 - iter 3/11 - loss 4.87369072 - samples/sec: 3.38 - lr: 0.000001\n",
            "2023-05-14 05:48:10,871 epoch 1 - iter 4/11 - loss 4.86396456 - samples/sec: 3.36 - lr: 0.000002\n",
            "2023-05-14 05:48:11,167 epoch 1 - iter 5/11 - loss 4.92793500 - samples/sec: 3.38 - lr: 0.000002\n",
            "2023-05-14 05:48:11,479 epoch 1 - iter 6/11 - loss 5.23548044 - samples/sec: 3.21 - lr: 0.000003\n",
            "2023-05-14 05:48:11,783 epoch 1 - iter 7/11 - loss 5.22582333 - samples/sec: 3.30 - lr: 0.000003\n",
            "2023-05-14 05:48:12,086 epoch 1 - iter 8/11 - loss 5.21822702 - samples/sec: 3.31 - lr: 0.000004\n",
            "2023-05-14 05:48:12,388 epoch 1 - iter 9/11 - loss 5.19513200 - samples/sec: 3.32 - lr: 0.000004\n",
            "2023-05-14 05:48:12,698 epoch 1 - iter 10/11 - loss 5.17885004 - samples/sec: 3.23 - lr: 0.000005\n",
            "2023-05-14 05:48:13,007 epoch 1 - iter 11/11 - loss 5.15185835 - samples/sec: 3.25 - lr: 0.000005\n",
            "2023-05-14 05:48:13,009 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:13,009 EPOCH 1 done: loss 5.1519 - lr 0.0000050\n",
            "2023-05-14 05:48:13,337 DEV : loss 5.167919158935547 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:13,337 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:13,340 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:13,432 epoch 2 - iter 1/11 - loss 5.35619392 - samples/sec: 11.32 - lr: 0.000005\n",
            "2023-05-14 05:48:13,743 epoch 2 - iter 2/11 - loss 4.98594157 - samples/sec: 3.21 - lr: 0.000005\n",
            "2023-05-14 05:48:14,054 epoch 2 - iter 3/11 - loss 5.31815602 - samples/sec: 3.22 - lr: 0.000005\n",
            "2023-05-14 05:48:14,355 epoch 2 - iter 4/11 - loss 5.39754221 - samples/sec: 3.32 - lr: 0.000005\n",
            "2023-05-14 05:48:14,661 epoch 2 - iter 5/11 - loss 5.34310896 - samples/sec: 3.28 - lr: 0.000005\n",
            "2023-05-14 05:48:14,964 epoch 2 - iter 6/11 - loss 5.30098063 - samples/sec: 3.31 - lr: 0.000005\n",
            "2023-05-14 05:48:15,264 epoch 2 - iter 7/11 - loss 5.27866888 - samples/sec: 3.34 - lr: 0.000005\n",
            "2023-05-14 05:48:15,564 epoch 2 - iter 8/11 - loss 5.29915705 - samples/sec: 3.35 - lr: 0.000005\n",
            "2023-05-14 05:48:15,868 epoch 2 - iter 9/11 - loss 5.29017615 - samples/sec: 3.29 - lr: 0.000005\n",
            "2023-05-14 05:48:16,183 epoch 2 - iter 10/11 - loss 5.32916861 - samples/sec: 3.18 - lr: 0.000004\n",
            "2023-05-14 05:48:16,478 epoch 2 - iter 11/11 - loss 5.35780635 - samples/sec: 3.40 - lr: 0.000004\n",
            "2023-05-14 05:48:16,480 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:16,481 EPOCH 2 done: loss 5.3578 - lr 0.0000044\n",
            "2023-05-14 05:48:16,792 DEV : loss 5.164629936218262 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:16,792 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:16,795 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:16,912 epoch 3 - iter 1/11 - loss 6.09002195 - samples/sec: 8.86 - lr: 0.000004\n",
            "2023-05-14 05:48:17,214 epoch 3 - iter 2/11 - loss 5.29808960 - samples/sec: 3.32 - lr: 0.000004\n",
            "2023-05-14 05:48:17,525 epoch 3 - iter 3/11 - loss 5.35840086 - samples/sec: 3.22 - lr: 0.000004\n",
            "2023-05-14 05:48:17,839 epoch 3 - iter 4/11 - loss 5.17366130 - samples/sec: 3.20 - lr: 0.000004\n",
            "2023-05-14 05:48:18,142 epoch 3 - iter 5/11 - loss 5.13304387 - samples/sec: 3.30 - lr: 0.000004\n",
            "2023-05-14 05:48:18,449 epoch 3 - iter 6/11 - loss 5.18921020 - samples/sec: 3.26 - lr: 0.000004\n",
            "2023-05-14 05:48:18,793 epoch 3 - iter 7/11 - loss 5.16300776 - samples/sec: 2.92 - lr: 0.000004\n",
            "2023-05-14 05:48:19,121 epoch 3 - iter 8/11 - loss 5.12807293 - samples/sec: 3.05 - lr: 0.000004\n",
            "2023-05-14 05:48:19,444 epoch 3 - iter 9/11 - loss 5.12066055 - samples/sec: 3.11 - lr: 0.000004\n",
            "2023-05-14 05:48:19,775 epoch 3 - iter 10/11 - loss 5.10096813 - samples/sec: 3.04 - lr: 0.000004\n",
            "2023-05-14 05:48:20,103 epoch 3 - iter 11/11 - loss 5.08841000 - samples/sec: 3.05 - lr: 0.000004\n",
            "2023-05-14 05:48:20,111 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:20,112 EPOCH 3 done: loss 5.0884 - lr 0.0000039\n",
            "2023-05-14 05:48:20,437 DEV : loss 5.147786617279053 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:20,437 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:20,440 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:20,561 epoch 4 - iter 1/11 - loss 4.77901035 - samples/sec: 8.51 - lr: 0.000004\n",
            "2023-05-14 05:48:20,896 epoch 4 - iter 2/11 - loss 4.94378738 - samples/sec: 2.99 - lr: 0.000004\n",
            "2023-05-14 05:48:21,218 epoch 4 - iter 3/11 - loss 4.89470554 - samples/sec: 3.12 - lr: 0.000004\n",
            "2023-05-14 05:48:21,550 epoch 4 - iter 4/11 - loss 4.81981493 - samples/sec: 3.04 - lr: 0.000004\n",
            "2023-05-14 05:48:21,889 epoch 4 - iter 5/11 - loss 4.78961957 - samples/sec: 2.96 - lr: 0.000004\n",
            "2023-05-14 05:48:22,212 epoch 4 - iter 6/11 - loss 4.78097077 - samples/sec: 3.12 - lr: 0.000004\n",
            "2023-05-14 05:48:22,556 epoch 4 - iter 7/11 - loss 5.03750926 - samples/sec: 2.91 - lr: 0.000004\n",
            "2023-05-14 05:48:22,902 epoch 4 - iter 8/11 - loss 5.12375594 - samples/sec: 2.90 - lr: 0.000003\n",
            "2023-05-14 05:48:23,242 epoch 4 - iter 9/11 - loss 5.11065875 - samples/sec: 2.94 - lr: 0.000003\n",
            "2023-05-14 05:48:23,588 epoch 4 - iter 10/11 - loss 5.11673773 - samples/sec: 2.90 - lr: 0.000003\n",
            "2023-05-14 05:48:23,923 epoch 4 - iter 11/11 - loss 5.13301679 - samples/sec: 2.99 - lr: 0.000003\n",
            "2023-05-14 05:48:23,927 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:23,927 EPOCH 4 done: loss 5.1330 - lr 0.0000033\n",
            "2023-05-14 05:48:24,327 DEV : loss 5.141849517822266 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:24,327 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:24,330 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:24,411 epoch 5 - iter 1/11 - loss 4.82766758 - samples/sec: 12.65 - lr: 0.000003\n",
            "2023-05-14 05:48:24,710 epoch 5 - iter 2/11 - loss 4.78666912 - samples/sec: 3.35 - lr: 0.000003\n",
            "2023-05-14 05:48:25,018 epoch 5 - iter 3/11 - loss 4.74895490 - samples/sec: 3.26 - lr: 0.000003\n",
            "2023-05-14 05:48:25,316 epoch 5 - iter 4/11 - loss 4.88732889 - samples/sec: 3.36 - lr: 0.000003\n",
            "2023-05-14 05:48:25,628 epoch 5 - iter 5/11 - loss 4.81734476 - samples/sec: 3.21 - lr: 0.000003\n",
            "2023-05-14 05:48:25,926 epoch 5 - iter 6/11 - loss 4.80285216 - samples/sec: 3.37 - lr: 0.000003\n",
            "2023-05-14 05:48:26,229 epoch 5 - iter 7/11 - loss 4.84196832 - samples/sec: 3.30 - lr: 0.000003\n",
            "2023-05-14 05:48:26,526 epoch 5 - iter 8/11 - loss 4.81093283 - samples/sec: 3.38 - lr: 0.000003\n",
            "2023-05-14 05:48:26,828 epoch 5 - iter 9/11 - loss 4.80329762 - samples/sec: 3.31 - lr: 0.000003\n",
            "2023-05-14 05:48:27,129 epoch 5 - iter 10/11 - loss 4.78288000 - samples/sec: 3.34 - lr: 0.000003\n",
            "2023-05-14 05:48:27,425 epoch 5 - iter 11/11 - loss 4.76925988 - samples/sec: 3.39 - lr: 0.000003\n",
            "2023-05-14 05:48:27,427 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:27,427 EPOCH 5 done: loss 4.7693 - lr 0.0000028\n",
            "2023-05-14 05:48:27,734 DEV : loss 5.1361799240112305 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:27,735 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:27,738 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:27,822 epoch 6 - iter 1/11 - loss 4.60795641 - samples/sec: 12.41 - lr: 0.000003\n",
            "2023-05-14 05:48:28,124 epoch 6 - iter 2/11 - loss 4.95347214 - samples/sec: 3.32 - lr: 0.000003\n",
            "2023-05-14 05:48:28,423 epoch 6 - iter 3/11 - loss 4.76438332 - samples/sec: 3.35 - lr: 0.000003\n",
            "2023-05-14 05:48:28,720 epoch 6 - iter 4/11 - loss 4.75148379 - samples/sec: 3.38 - lr: 0.000003\n",
            "2023-05-14 05:48:29,036 epoch 6 - iter 5/11 - loss 4.76012862 - samples/sec: 3.17 - lr: 0.000003\n",
            "2023-05-14 05:48:29,337 epoch 6 - iter 6/11 - loss 4.90433963 - samples/sec: 3.34 - lr: 0.000002\n",
            "2023-05-14 05:48:29,633 epoch 6 - iter 7/11 - loss 4.90465028 - samples/sec: 3.38 - lr: 0.000002\n",
            "2023-05-14 05:48:29,937 epoch 6 - iter 8/11 - loss 4.91178071 - samples/sec: 3.29 - lr: 0.000002\n",
            "2023-05-14 05:48:30,243 epoch 6 - iter 9/11 - loss 4.96481243 - samples/sec: 3.28 - lr: 0.000002\n",
            "2023-05-14 05:48:30,536 epoch 6 - iter 10/11 - loss 4.94854131 - samples/sec: 3.42 - lr: 0.000002\n",
            "2023-05-14 05:48:30,835 epoch 6 - iter 11/11 - loss 4.96705884 - samples/sec: 3.35 - lr: 0.000002\n",
            "2023-05-14 05:48:30,837 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:30,837 EPOCH 6 done: loss 4.9671 - lr 0.0000022\n",
            "2023-05-14 05:48:31,145 DEV : loss 5.107648849487305 - f1-score (micro avg)  0.0\n",
            "2023-05-14 05:48:31,145 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:31,150 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:31,248 epoch 7 - iter 1/11 - loss 5.29792881 - samples/sec: 10.45 - lr: 0.000002\n",
            "2023-05-14 05:48:31,544 epoch 7 - iter 2/11 - loss 5.18267810 - samples/sec: 3.38 - lr: 0.000002\n",
            "2023-05-14 05:48:31,848 epoch 7 - iter 3/11 - loss 5.12362844 - samples/sec: 3.30 - lr: 0.000002\n",
            "2023-05-14 05:48:32,157 epoch 7 - iter 4/11 - loss 5.04252696 - samples/sec: 3.25 - lr: 0.000002\n",
            "2023-05-14 05:48:32,452 epoch 7 - iter 5/11 - loss 5.00125723 - samples/sec: 3.39 - lr: 0.000002\n",
            "2023-05-14 05:48:32,752 epoch 7 - iter 6/11 - loss 4.92278956 - samples/sec: 3.35 - lr: 0.000002\n",
            "2023-05-14 05:48:33,052 epoch 7 - iter 7/11 - loss 4.82248292 - samples/sec: 3.33 - lr: 0.000002\n",
            "2023-05-14 05:48:33,353 epoch 7 - iter 8/11 - loss 4.80316041 - samples/sec: 3.33 - lr: 0.000002\n",
            "2023-05-14 05:48:33,652 epoch 7 - iter 9/11 - loss 4.77636256 - samples/sec: 3.35 - lr: 0.000002\n",
            "2023-05-14 05:48:33,967 epoch 7 - iter 10/11 - loss 4.76075876 - samples/sec: 3.18 - lr: 0.000002\n",
            "2023-05-14 05:48:34,268 epoch 7 - iter 11/11 - loss 4.75513107 - samples/sec: 3.34 - lr: 0.000002\n",
            "2023-05-14 05:48:34,272 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:34,272 EPOCH 7 done: loss 4.7551 - lr 0.0000017\n",
            "2023-05-14 05:48:34,609 DEV : loss 5.08971643447876 - f1-score (micro avg)  0.3333\n",
            "2023-05-14 05:48:34,609 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:34,612 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:34,723 epoch 8 - iter 1/11 - loss 4.56678867 - samples/sec: 9.28 - lr: 0.000002\n",
            "2023-05-14 05:48:35,043 epoch 8 - iter 2/11 - loss 5.18353443 - samples/sec: 3.13 - lr: 0.000002\n",
            "2023-05-14 05:48:35,370 epoch 8 - iter 3/11 - loss 5.25684492 - samples/sec: 3.06 - lr: 0.000002\n",
            "2023-05-14 05:48:35,692 epoch 8 - iter 4/11 - loss 5.12543717 - samples/sec: 3.11 - lr: 0.000001\n",
            "2023-05-14 05:48:36,016 epoch 8 - iter 5/11 - loss 5.20181646 - samples/sec: 3.10 - lr: 0.000001\n",
            "2023-05-14 05:48:36,341 epoch 8 - iter 6/11 - loss 5.15056619 - samples/sec: 3.09 - lr: 0.000001\n",
            "2023-05-14 05:48:36,666 epoch 8 - iter 7/11 - loss 5.14447078 - samples/sec: 3.08 - lr: 0.000001\n",
            "2023-05-14 05:48:37,024 epoch 8 - iter 8/11 - loss 5.10842268 - samples/sec: 2.80 - lr: 0.000001\n",
            "2023-05-14 05:48:37,360 epoch 8 - iter 9/11 - loss 5.13983071 - samples/sec: 3.03 - lr: 0.000001\n",
            "2023-05-14 05:48:37,688 epoch 8 - iter 10/11 - loss 5.10234020 - samples/sec: 3.05 - lr: 0.000001\n",
            "2023-05-14 05:48:38,021 epoch 8 - iter 11/11 - loss 5.03155792 - samples/sec: 3.01 - lr: 0.000001\n",
            "2023-05-14 05:48:38,024 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:38,024 EPOCH 8 done: loss 5.0316 - lr 0.0000011\n",
            "2023-05-14 05:48:38,375 DEV : loss 5.071990489959717 - f1-score (micro avg)  0.3333\n",
            "2023-05-14 05:48:38,376 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:38,380 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:38,509 epoch 9 - iter 1/11 - loss 5.57239012 - samples/sec: 7.95 - lr: 0.000001\n",
            "2023-05-14 05:48:38,849 epoch 9 - iter 2/11 - loss 5.18373544 - samples/sec: 2.95 - lr: 0.000001\n",
            "2023-05-14 05:48:39,191 epoch 9 - iter 3/11 - loss 5.01690551 - samples/sec: 2.93 - lr: 0.000001\n",
            "2023-05-14 05:48:39,543 epoch 9 - iter 4/11 - loss 5.05273412 - samples/sec: 2.85 - lr: 0.000001\n",
            "2023-05-14 05:48:39,884 epoch 9 - iter 5/11 - loss 4.94047861 - samples/sec: 2.93 - lr: 0.000001\n",
            "2023-05-14 05:48:40,177 epoch 9 - iter 6/11 - loss 4.93576215 - samples/sec: 3.42 - lr: 0.000001\n",
            "2023-05-14 05:48:40,485 epoch 9 - iter 7/11 - loss 4.86072766 - samples/sec: 3.26 - lr: 0.000001\n",
            "2023-05-14 05:48:40,782 epoch 9 - iter 8/11 - loss 4.88196520 - samples/sec: 3.37 - lr: 0.000001\n",
            "2023-05-14 05:48:41,082 epoch 9 - iter 9/11 - loss 4.86503383 - samples/sec: 3.34 - lr: 0.000001\n",
            "2023-05-14 05:48:41,388 epoch 9 - iter 10/11 - loss 4.83776340 - samples/sec: 3.28 - lr: 0.000001\n",
            "2023-05-14 05:48:41,686 epoch 9 - iter 11/11 - loss 4.82070551 - samples/sec: 3.37 - lr: 0.000001\n",
            "2023-05-14 05:48:41,688 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:41,688 EPOCH 9 done: loss 4.8207 - lr 0.0000006\n",
            "2023-05-14 05:48:41,990 DEV : loss 5.058379650115967 - f1-score (micro avg)  0.3333\n",
            "2023-05-14 05:48:41,991 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:41,993 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:42,080 epoch 10 - iter 1/11 - loss 4.64404602 - samples/sec: 11.85 - lr: 0.000001\n",
            "2023-05-14 05:48:42,382 epoch 10 - iter 2/11 - loss 4.51923964 - samples/sec: 3.32 - lr: 0.000000\n",
            "2023-05-14 05:48:42,683 epoch 10 - iter 3/11 - loss 4.74994190 - samples/sec: 3.33 - lr: 0.000000\n",
            "2023-05-14 05:48:42,989 epoch 10 - iter 4/11 - loss 4.74035768 - samples/sec: 3.27 - lr: 0.000000\n",
            "2023-05-14 05:48:43,293 epoch 10 - iter 5/11 - loss 5.02045845 - samples/sec: 3.29 - lr: 0.000000\n",
            "2023-05-14 05:48:43,607 epoch 10 - iter 6/11 - loss 4.98752409 - samples/sec: 3.20 - lr: 0.000000\n",
            "2023-05-14 05:48:43,906 epoch 10 - iter 7/11 - loss 4.94877673 - samples/sec: 3.35 - lr: 0.000000\n",
            "2023-05-14 05:48:44,216 epoch 10 - iter 8/11 - loss 5.01035067 - samples/sec: 3.23 - lr: 0.000000\n",
            "2023-05-14 05:48:44,514 epoch 10 - iter 9/11 - loss 4.99242238 - samples/sec: 3.36 - lr: 0.000000\n",
            "2023-05-14 05:48:44,815 epoch 10 - iter 10/11 - loss 5.02536959 - samples/sec: 3.33 - lr: 0.000000\n",
            "2023-05-14 05:48:45,110 epoch 10 - iter 11/11 - loss 5.02567623 - samples/sec: 3.40 - lr: 0.000000\n",
            "2023-05-14 05:48:45,112 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:45,112 EPOCH 10 done: loss 5.0257 - lr 0.0000000\n",
            "2023-05-14 05:48:45,411 DEV : loss 5.052210330963135 - f1-score (micro avg)  0.3333\n",
            "2023-05-14 05:48:45,411 BAD EPOCHS (no improvement): 4\n",
            "2023-05-14 05:48:55,339 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:55,342 Testing using last state of model ...\n",
            "2023-05-14 05:48:55,448 0.0\t0.0\t0.0\t0.0\n",
            "2023-05-14 05:48:55,449 \n",
            "Results:\n",
            "- F-score (micro) 0.0\n",
            "- F-score (macro) 0.0\n",
            "- Accuracy 0.0\n",
            "\n",
            "By class:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          was     0.0000    0.0000    0.0000         0\n",
            "          our     0.0000    0.0000    0.0000         1\n",
            "   Principles     0.0000    0.0000    0.0000         1\n",
            "    Lordships     0.0000    0.0000    0.0000         1\n",
            "         para     0.0000    0.0000    0.0000         1\n",
            "          for     0.0000    0.0000    0.0000         1\n",
            "      section     0.0000    0.0000    0.0000         1\n",
            "          the     0.0000    0.0000    0.0000         1\n",
            "          LoA     0.0000    0.0000    0.0000         1\n",
            "     complete     0.0000    0.0000    0.0000         0\n",
            "Investigating     0.0000    0.0000    0.0000         0\n",
            "  Bhanwarial,     0.0000    0.0000    0.0000         0\n",
            "         that     0.0000    0.0000    0.0000         0\n",
            "           10     0.0000    0.0000    0.0000         0\n",
            "\n",
            "    micro avg     0.0000    0.0000    0.0000         8\n",
            "    macro avg     0.0000    0.0000    0.0000         8\n",
            " weighted avg     0.0000    0.0000    0.0000         8\n",
            "  samples avg     0.0000    0.0000    0.0000         8\n",
            "\n",
            "2023-05-14 05:48:55,449 ----------------------------------------------------------------------------------------------------\n",
            "2023-05-14 05:48:55,711 loading file resources/taggers/sota-ner-flert-xlm/final-model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4rsS_JMPzIz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}